{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Module 3 - LLM Fundamentals Assessment\n\n**Total Points: 20**\n\n## Assessment Instructions\n\n1. Complete all exercises in this notebook\n2. Do NOT rename any functions - the grader expects specific names\n3. Ensure your notebook runs without errors before submission\n4. Use the LLM Gateway configuration provided by your instructor\n\n## LLM Gateway Setup\n\nConfigure your LLM endpoint below using the values provided by your instructor."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# ===== LLM GATEWAY CONFIGURATION =====\n# Set these values as provided by your instructor\n\nLLM_BASE_URL = \"https://jbchat.jonbowden.com.ngrok.app\"  # JBChat gateway endpoint\nLLM_API_KEY = \"<provided-by-instructor>\"  # Optional - leave as-is if not required\n\nDEFAULT_MODEL = \"phi3:mini\"\n\nimport requests\nimport json"
  },
  {
   "cell_type": "markdown",
   "id": "ex1-header",
   "metadata": {},
   "source": "---\n## Exercise 1 - Basic LLM Caller (4 points)\n\nCreate a function `call_llm` that:\n- Takes parameters: `prompt` (str), `temperature` (float, default 0.0)\n- Calls the `/api/chat` endpoint with the configured `LLM_BASE_URL`\n- Sends a JSON payload with keys: `model`, `messages` (array with user message), `temperature`, `stream` (set to False)\n- Includes headers: `Content-Type`, `ngrok-skip-browser-warning`\n- Returns the response text (extracted from `message.content`)\n\n**Requirements:**\n- Use `requests.post()` to make the HTTP call\n- Use `DEFAULT_MODEL` for the model name\n- Format messages as: `[{\"role\": \"user\", \"content\": prompt}]`\n- Return the text content, not the full JSON response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-code",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1: Implement call_llm function\n\ndef call_llm(prompt: str, temperature: float = 0.0) -> str:\n    \"\"\"Call an LLM endpoint via /api/chat and return the response text.\"\"\"\n    # YOUR CODE HERE\n    pass\n\n# Test your function (optional)\n# result = call_llm(\"Say hello in one word.\")\n# print(result)"
  },
  {
   "cell_type": "markdown",
   "id": "ex2-header",
   "metadata": {},
   "source": "---\n## Exercise 2 - Extract Response Text (3 points)\n\nCreate a function `get_response_text` that:\n- Takes `prompt` (str) as input\n- Calls `call_llm` with temperature=0.0\n- Returns the response text (your `call_llm` should already return text)\n- Returns an empty string if an error occurs\n\n**Requirements:**\n- Must use your `call_llm` function\n- Handle any exceptions gracefully\n- Return type must be `str`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-code",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 2: Implement get_response_text function\n\ndef get_response_text(prompt: str) -> str:\n    \"\"\"Call LLM and return just the response text.\"\"\"\n    # YOUR CODE HERE\n    pass\n\n# Test your function (optional)\n# text = get_response_text(\"What is 2+2? Answer with just the number.\")\n# print(text)"
  },
  {
   "cell_type": "markdown",
   "id": "ex3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3 - JSON Output Parser (4 points)\n",
    "\n",
    "Create a function `parse_json_response` that:\n",
    "- Takes `prompt` (str) as input\n",
    "- Calls `get_response_text` to get the LLM output\n",
    "- Attempts to parse the response as JSON\n",
    "- Returns a tuple: `(success: bool, result: dict or str)`\n",
    "  - If parsing succeeds: `(True, parsed_dict)`\n",
    "  - If parsing fails: `(False, error_message)`\n",
    "\n",
    "**Requirements:**\n",
    "- Use `json.loads()` for parsing\n",
    "- Use try/except to handle `json.JSONDecodeError`\n",
    "- The error message should describe the failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement parse_json_response function\n",
    "\n",
    "def parse_json_response(prompt: str) -> tuple:\n",
    "    \"\"\"Call LLM, attempt to parse response as JSON, return (success, result).\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your function (optional)\n",
    "# success, result = parse_json_response('Return ONLY valid JSON: {\"test\": 123}')\n",
    "# print(f\"Success: {success}, Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-header",
   "metadata": {},
   "source": "---\n## Exercise 4 - Temperature Comparison (4 points)\n\nCreate a function `compare_temperatures` that:\n- Takes `prompt` (str) as input\n- Calls `call_llm` twice with the SAME prompt:\n  - Once with temperature=0.0\n  - Once with temperature=0.8\n- Returns a dictionary with keys:\n  - `\"low_temp\"`: response text from temperature=0.0\n  - `\"high_temp\"`: response text from temperature=0.8\n  - `\"are_identical\"`: boolean indicating if both responses are exactly the same\n\n**Requirements:**\n- Use your `call_llm` function (which returns text directly)\n- Compare strings exactly for `are_identical`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Implement compare_temperatures function\n",
    "\n",
    "def compare_temperatures(prompt: str) -> dict:\n",
    "    \"\"\"Compare LLM responses at different temperatures.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your function (optional)\n",
    "# result = compare_temperatures(\"List 3 fruits.\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 5 - Structured Prompt Builder (5 points)\n",
    "\n",
    "Create a function `build_structured_prompt` that:\n",
    "- Takes parameters:\n",
    "  - `system_instruction` (str): behavior rules for the LLM\n",
    "  - `task` (str): what the LLM should do\n",
    "  - `constraints` (list of str): output constraints\n",
    "- Returns a formatted prompt string with clearly labeled sections:\n",
    "  ```\n",
    "  SYSTEM:\n",
    "  {system_instruction}\n",
    "  \n",
    "  TASK:\n",
    "  {task}\n",
    "  \n",
    "  CONSTRAINTS:\n",
    "  - {constraint1}\n",
    "  - {constraint2}\n",
    "  ```\n",
    "\n",
    "**Requirements:**\n",
    "- Each constraint should be on its own line, prefixed with `\"- \"`\n",
    "- Sections must be separated by blank lines\n",
    "- Use uppercase labels: SYSTEM, TASK, CONSTRAINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Implement build_structured_prompt function\n",
    "\n",
    "def build_structured_prompt(system_instruction: str, task: str, constraints: list) -> str:\n",
    "    \"\"\"Build a structured prompt with system, task, and constraints.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your function (optional)\n",
    "# prompt = build_structured_prompt(\n",
    "#     \"You are a helpful assistant.\",\n",
    "#     \"Explain Python lists.\",\n",
    "#     [\"Use simple language\", \"Maximum 2 sentences\"]\n",
    "# )\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission\n",
    "\n",
    "Before submitting:\n",
    "1. **Restart kernel** and **Run All Cells** to ensure everything works\n",
    "2. Verify all functions are defined and return correct types\n",
    "3. Save the notebook\n",
    "\n",
    "Submit your completed notebook via the [Module 3 Assessment Form](https://forms.google.com/MODULE3_FORM_LINK)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}