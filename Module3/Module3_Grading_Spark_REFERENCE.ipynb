{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 3 \u2013 Automated Grading Notebook (Spark LLM)\n",
        "\n",
        "## Purpose\n",
        "This notebook is used by markers to grade **Module 3** submissions.\n",
        "\n",
        "It explicitly aligns to the Module 3 rubric and uses the Spark-hosted LLM **as a simulation engine**, not a source of truth.\n",
        "\n",
        "### Key Principles\n",
        "- Prompt discipline is graded independently of model behaviour\n",
        "- Malformed JSON handling **is assessed explicitly**\n",
        "- Live LLM calls validate robustness, not semantic correctness\n",
        "- All scoring paths are deterministic and appeal-safe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rubric Alignment\n",
        "\n",
        "| Rubric Area | Weight | Assessed How |\n",
        "|------------|--------|--------------|\n",
        "| Prompt discipline | 35% | Static prompt inspection |\n",
        "| API usage & parameters | 15% | Code inspection + runtime |\n",
        "| JSON robustness | 30% | Injected malformed responses |\n",
        "| Graceful failure handling | 20% | Live Spark call tolerance |\n",
        "| **Total** | **100%** | |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Marker Instructions\n",
        "\n",
        "1. Run all cells top-to-bottom\n",
        "2. Do **not** modify scoring logic\n",
        "3. Review commentary cells for borderline cases\n",
        "4. Record final score and notes externally\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# --- Spark API Client (Provided) ---\n",
        "import requests\n",
        "import json\n",
        "from typing import Any, Dict\n",
        "\n",
        "SPARK_BASE_URL = \"http://spark:11434\"\n",
        "MODEL = \"phi3:mini\"\n",
        "\n",
        "def call_spark_llm(prompt: str, temperature: float = 0.0) -> str:\n",
        "    payload = {\n",
        "        \"model\": MODEL,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": temperature,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    r = requests.post(f\"{SPARK_BASE_URL}/api/chat\", json=payload, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    return r.json()[\"message\"][\"content\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Student Submission\n",
        "\n",
        "**Markers:** paste the student\u2019s prompt and parsing function below exactly as submitted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# --- Student Prompt ---\n",
        "student_prompt = \"\"\"\n",
        "PASTE STUDENT PROMPT HERE\n",
        "\"\"\"\n",
        "\n",
        "# --- Student Parsing Function ---\n",
        "def parse_llm_response(text: str) -> Dict[str, Any]:\n",
        "    # PASTE STUDENT FUNCTION HERE\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prompt Discipline (35%)\n",
        "\n",
        "**Marker commentary:**\n",
        "- Does the prompt explicitly require JSON?\n",
        "- Does it define keys / schema?\n",
        "- Does it prohibit extra text?\n",
        "- Are examples or constraints provided?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "def grade_prompt(prompt: str) -> int:\n",
        "    score = 0\n",
        "    p = prompt.lower()\n",
        "\n",
        "    if \"json\" in p:\n",
        "        score += 10\n",
        "    if \"key\" in p or \"schema\" in p:\n",
        "        score += 10\n",
        "    if \"no extra\" in p or \"only\" in p:\n",
        "        score += 5\n",
        "    if \"example\" in p:\n",
        "        score += 10\n",
        "\n",
        "    return min(score, 35)\n",
        "\n",
        "prompt_score = grade_prompt(student_prompt)\n",
        "prompt_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. JSON Robustness \u2013 Injected Tests (30%)\n",
        "\n",
        "**Marker commentary:**\n",
        "- Students are expected to handle malformed JSON\n",
        "- Failures here reflect insufficient defensive parsing\n",
        "- This does NOT depend on live model output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "MALFORMED_RESPONSES = [\n",
        "    '{\"answer\": \"yes\", \"confidence\": 0.8',\n",
        "    '```json {\"answer\": \"yes\"} ```',\n",
        "    '{\"answer\": \"yes\"} trailing text',\n",
        "    '{\"confidence\": 0.9}'\n",
        "]\n",
        "\n",
        "def grade_parsing(fn) -> int:\n",
        "    score = 0\n",
        "    for text in MALFORMED_RESPONSES:\n",
        "        try:\n",
        "            result = fn(text)\n",
        "            if isinstance(result, dict):\n",
        "                score += 7.5\n",
        "        except Exception:\n",
        "            pass\n",
        "    return int(min(score, 30))\n",
        "\n",
        "parsing_score = grade_parsing(parse_llm_response)\n",
        "parsing_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Live Spark Robustness Check (20%)\n",
        "\n",
        "**Marker commentary:**\n",
        "- Live LLM output is used ONLY to validate robustness\n",
        "- Semantic correctness is NOT graded\n",
        "- Failure must be due to student code, not wording variance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "def grade_live_behavior(prompt: str, parse_fn) -> int:\n",
        "    try:\n",
        "        raw = call_spark_llm(prompt)\n",
        "        parsed = parse_fn(raw)\n",
        "        if isinstance(parsed, dict):\n",
        "            return 20\n",
        "    except Exception:\n",
        "        return 0\n",
        "    return 0\n",
        "\n",
        "live_score = grade_live_behavior(student_prompt, parse_llm_response)\n",
        "live_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Score\n",
        "\n",
        "**Marker commentary:**\n",
        "- Use this total as the official Module 3 mark\n",
        "- Record qualitative notes separately if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "total_score = prompt_score + parsing_score + live_score\n",
        "total_score\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}