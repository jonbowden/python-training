{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 3 - LLM Fundamentals Assessment\n",
    "\n",
    "**TEST SUBMISSION - For grading pipeline testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLM GATEWAY CONFIGURATION =====\n",
    "LLM_BASE_URL = \"http://localhost:11434\"\n",
    "LLM_API_KEY = None\n",
    "DEFAULT_MODEL = \"phi3:mini\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement call_llm function\n",
    "\n",
    "def call_llm(prompt: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"Call an LLM endpoint and return the response text.\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"ngrok-skip-browser-warning\": \"true\",\n",
    "        \"Bypass-Tunnel-Reminder\": \"true\",\n",
    "    }\n",
    "    \n",
    "    if LLM_API_KEY:\n",
    "        headers[\"X-API-Key\"] = LLM_API_KEY\n",
    "        endpoint = f\"{LLM_BASE_URL.rstrip('/')}/chat/direct\"\n",
    "    else:\n",
    "        endpoint = f\"{LLM_BASE_URL.rstrip('/')}/api/chat\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": DEFAULT_MODEL,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    resp = requests.post(endpoint, headers=headers, json=payload, timeout=(10, 120))\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    \n",
    "    return data[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Implement get_response_text function\n",
    "\n",
    "def get_response_text(prompt: str) -> str:\n",
    "    \"\"\"Call LLM and return just the response text.\"\"\"\n",
    "    try:\n",
    "        return call_llm(prompt, temperature=0.0)\n",
    "    except Exception:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement parse_json_response function\n",
    "\n",
    "def parse_json_response(prompt: str) -> tuple:\n",
    "    \"\"\"Call LLM, attempt to parse response as JSON, return (success, result).\"\"\"\n",
    "    raw = get_response_text(prompt)\n",
    "    \n",
    "    # Strip markdown code blocks\n",
    "    cleaned = raw.strip()\n",
    "    pattern = r'^```(?:json)?\\s*\\n?(.*?)\\n?```$'\n",
    "    match = re.match(pattern, cleaned, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        cleaned = match.group(1).strip()\n",
    "    \n",
    "    try:\n",
    "        return (True, json.loads(cleaned))\n",
    "    except json.JSONDecodeError as e:\n",
    "        return (False, f\"JSONDecodeError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Implement compare_temperatures function\n",
    "\n",
    "def compare_temperatures(prompt: str) -> dict:\n",
    "    \"\"\"Compare LLM responses at different temperatures.\"\"\"\n",
    "    low = call_llm(prompt, temperature=0.0)\n",
    "    high = call_llm(prompt, temperature=0.8)\n",
    "    \n",
    "    return {\n",
    "        \"low_temp\": low,\n",
    "        \"high_temp\": high,\n",
    "        \"are_identical\": low == high\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Implement build_structured_prompt function\n",
    "\n",
    "def build_structured_prompt(system_instruction: str, task: str, constraints: list) -> str:\n",
    "    \"\"\"Build a structured prompt with system, task, and constraints.\"\"\"\n",
    "    constraint_lines = \"\\n\".join([f\"- {c}\" for c in constraints])\n",
    "    \n",
    "    return f\"\"\"SYSTEM:\n{system_instruction}\n\nTASK:\n{task}\n\nCONSTRAINTS:\n{constraint_lines}\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
