{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 3 - LLM Fundamentals Assessment (Instructor Version)\n",
    "\n",
    "**INSTRUCTOR / GRADING TEMPLATE**\n",
    "\n",
    "This notebook contains **hidden assessment logic** and must NOT be shared with students.\n",
    "\n",
    "Purpose:\n",
    "- Inject student code programmatically\n",
    "- Run automated tests\n",
    "- Produce authoritative scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-scoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN: SCORING SETUP ===\n",
    "__assessment_scores = {}\n",
    "__assessment_feedback = {}\n",
    "\n",
    "def record_score(exercise, points, max_points, feedback=None):\n",
    "    __assessment_scores[exercise] = (points, max_points)\n",
    "    if feedback:\n",
    "        __assessment_feedback[exercise] = feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-config",
   "metadata": {},
   "outputs": [],
   "source": "# === GRADING ENVIRONMENT CONFIG ===\n# Mock LLM for grading - no actual LLM available on GitHub Actions\n\nLLM_BASE_URL = \"http://mock-llm-for-grading\"\nLLM_API_KEY = None\nDEFAULT_MODEL = \"phi3:mini\"\n\nimport requests\nimport json\nimport re\n\n# === MOCK LLM SETUP ===\n# We inject a mock requests.post that returns predictable responses\n# This allows testing student code structure without a real LLM\n\n_original_post = requests.post\n_mock_call_count = 0\n\ndef _mock_post(url, **kwargs):\n    \"\"\"Mock requests.post for LLM endpoints.\"\"\"\n    global _mock_call_count\n    _mock_call_count += 1\n    \n    # Check if this is an LLM call\n    if '/api/chat' in url or '/chat/direct' in url:\n        payload = kwargs.get('json', {})\n        messages = payload.get('messages', [])\n        prompt = messages[0].get('content', '') if messages else ''\n        temperature = payload.get('temperature', 0.0)\n        \n        # Generate predictable mock responses based on prompt content\n        if 'json' in prompt.lower() or '{' in prompt:\n            # Return JSON-like response (sometimes wrapped in markdown)\n            if _mock_call_count % 2 == 0:\n                response_text = '```json\\n{\"test\": 1, \"value\": 42}\\n```'\n            else:\n                response_text = '{\"test\": 1, \"value\": 42}'\n        elif 'hello' in prompt.lower():\n            response_text = \"Hello!\"\n        elif 'color' in prompt.lower() or 'fruit' in prompt.lower():\n            if temperature > 0.5:\n                response_text = \"Red, Blue, Green (high temp response)\"\n            else:\n                response_text = \"Red, Blue, Green\"\n        else:\n            response_text = f\"Mock LLM response for: {prompt[:50]}...\"\n        \n        # Create mock response object\n        class MockResponse:\n            status_code = 200\n            def raise_for_status(self):\n                pass\n            def json(self):\n                return {\"message\": {\"content\": response_text}}\n        \n        return MockResponse()\n    \n    # For non-LLM calls, use original requests.post\n    return _original_post(url, **kwargs)\n\n# Monkey-patch requests.post\nrequests.post = _mock_post"
  },
  {
   "cell_type": "markdown",
   "id": "ex1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 - Basic LLM Caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 1 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'call_llm' in dir() or 'call_llm' in globals(), \"Function 'call_llm' not defined\"\n    assert callable(call_llm), \"'call_llm' should be a function\"\n    points += 1\n    feedback.append(\"✓ Function 'call_llm' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking function: {type(e).__name__}\")\n\ntry:\n    result = call_llm(\"Say hello in one word.\")\n    assert isinstance(result, str), f\"Should return str, got {type(result).__name__}\"\n    points += 1\n    feedback.append(\"✓ Function returns a string\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error calling function: {type(e).__name__}: {e}\")\n\ntry:\n    result = call_llm(\"Say hello in one word.\")\n    assert len(result) > 0, \"Response should not be empty\"\n    points += 1\n    feedback.append(f\"✓ Response contains text ({len(result)} chars)\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking response: {type(e).__name__}\")\n\ntry:\n    result = call_llm(\"Say hello in one word.\", temperature=0.5)\n    assert isinstance(result, str), \"Should work with temperature parameter\"\n    points += 1\n    feedback.append(\"✓ Temperature parameter works\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error with temperature param: {type(e).__name__}\")\n\nrecord_score('Exercise 1', points, 4, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "ex2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 - Extract Response Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 2 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'get_response_text' in dir() or 'get_response_text' in globals(), \"Function 'get_response_text' not defined\"\n    assert callable(get_response_text), \"'get_response_text' should be a function\"\n    points += 1\n    feedback.append(\"✓ Function 'get_response_text' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking function: {type(e).__name__}\")\n\ntry:\n    result = get_response_text(\"Say hello\")\n    assert isinstance(result, str), f\"Should return str, got {type(result).__name__}\"\n    points += 1\n    feedback.append(\"✓ Function returns a string\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error calling function: {type(e).__name__}\")\n\ntry:\n    result = get_response_text(\"Say hello\")\n    assert len(result) > 0, \"Response should not be empty\"\n    points += 1\n    feedback.append(f\"✓ Function returns non-empty text ({len(result)} chars)\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking response: {type(e).__name__}\")\n\nrecord_score('Exercise 2', points, 3, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "ex3-header",
   "metadata": {},
   "source": [
    "## Exercise 3 - JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 3 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'parse_json_response' in dir() or 'parse_json_response' in globals(), \"Function 'parse_json_response' not defined\"\n    assert callable(parse_json_response), \"'parse_json_response' should be a function\"\n    points += 1\n    feedback.append(\"✓ Function 'parse_json_response' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking function: {type(e).__name__}\")\n\ntry:\n    result = parse_json_response('Return ONLY this JSON: {\"test\": 1}')\n    assert isinstance(result, tuple), f\"Should return tuple, got {type(result).__name__}\"\n    assert len(result) == 2, f\"Tuple should have 2 elements, got {len(result)}\"\n    points += 1\n    feedback.append(\"✓ Function returns a tuple with 2 elements\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error calling function: {type(e).__name__}\")\n\ntry:\n    success, result = parse_json_response('Return ONLY this JSON: {\"test\": 1}')\n    assert isinstance(success, bool), f\"First element should be bool, got {type(success).__name__}\"\n    points += 1\n    feedback.append(\"✓ First tuple element is a boolean\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking tuple: {type(e).__name__}\")\n\ntry:\n    success, result = parse_json_response('Return ONLY this exact JSON, no other text: {\"value\": 42}')\n    if success:\n        assert isinstance(result, dict), \"On success, second element should be dict\"\n        points += 1\n        feedback.append(\"✓ Correctly parses JSON response (handles markdown wrapping)\")\n    else:\n        points += 0.5\n        feedback.append(f\"✗ JSON parsing failed: {result}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error in JSON test: {type(e).__name__}\")\n\nrecord_score('Exercise 3', points, 4, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "ex4-header",
   "metadata": {},
   "source": [
    "## Exercise 4 - Temperature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 4 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'compare_temperatures' in dir() or 'compare_temperatures' in globals(), \"Function 'compare_temperatures' not defined\"\n    assert callable(compare_temperatures), \"'compare_temperatures' should be a function\"\n    points += 1\n    feedback.append(\"✓ Function 'compare_temperatures' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking function: {type(e).__name__}\")\n\ntry:\n    result = compare_temperatures(\"List 3 colors.\")\n    assert isinstance(result, dict), f\"Should return dict, got {type(result).__name__}\"\n    points += 1\n    feedback.append(\"✓ Function returns a dictionary\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error calling function: {type(e).__name__}: {e}\")\n\ntry:\n    result = compare_temperatures(\"List 3 colors.\")\n    assert 'low_temp' in result, \"Dict should have 'low_temp' key\"\n    assert 'high_temp' in result, \"Dict should have 'high_temp' key\"\n    assert 'are_identical' in result, \"Dict should have 'are_identical' key\"\n    points += 1\n    feedback.append(\"✓ Dictionary has all required keys\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking keys: {type(e).__name__}\")\n\ntry:\n    result = compare_temperatures(\"List 3 colors.\")\n    assert isinstance(result['are_identical'], bool), \"'are_identical' should be boolean\"\n    assert isinstance(result['low_temp'], str), \"'low_temp' should be string\"\n    assert isinstance(result['high_temp'], str), \"'high_temp' should be string\"\n    points += 1\n    feedback.append(\"✓ All values have correct types\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking types: {type(e).__name__}\")\n\nrecord_score('Exercise 4', points, 4, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "ex5-header",
   "metadata": {},
   "source": [
    "## Exercise 5 - Structured Prompt Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 5 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'build_structured_prompt' in dir() or 'build_structured_prompt' in globals(), \"Function 'build_structured_prompt' not defined\"\n    assert callable(build_structured_prompt), \"'build_structured_prompt' should be a function\"\n    points += 1\n    feedback.append(\"✓ Function 'build_structured_prompt' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking function: {type(e).__name__}\")\n\ntry:\n    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Keep it short\"])\n    assert isinstance(result, str), f\"Should return str, got {type(result).__name__}\"\n    points += 1\n    feedback.append(\"✓ Function returns a string\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error calling function: {type(e).__name__}\")\n\ntry:\n    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Keep it short\"])\n    assert 'SYSTEM:' in result, \"Should contain 'SYSTEM:' label\"\n    assert 'TASK:' in result, \"Should contain 'TASK:' label\"\n    assert 'CONSTRAINTS:' in result, \"Should contain 'CONSTRAINTS:' label\"\n    points += 1\n    feedback.append(\"✓ Prompt contains all section labels\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking labels: {type(e).__name__}\")\n\ntry:\n    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Short\", \"Clear\"])\n    assert '- Short' in result or '- short' in result.lower(), \"Constraints should be prefixed with '- '\"\n    assert '- Clear' in result or '- clear' in result.lower(), \"Each constraint should have '- ' prefix\"\n    points += 1\n    feedback.append(\"✓ Constraints are properly formatted with '- ' prefix\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking constraints: {type(e).__name__}\")\n\ntry:\n    result = build_structured_prompt(\"Test system\", \"Test task\", [\"C1\", \"C2\"])\n    assert 'Test system' in result, \"System instruction should be in output\"\n    assert 'Test task' in result, \"Task should be in output\"\n    points += 1\n    feedback.append(\"✓ All inputs are included in output\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking content: {type(e).__name__}\")\n\nrecord_score('Exercise 5', points, 5, feedback)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN: WRITE RESULTS ===\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "result = {\n",
    "    'scores': __assessment_scores,\n",
    "    'feedback': __assessment_feedback,\n",
    "    'timestamp': datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('assessment_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(\"Assessment Results:\")\n",
    "total = sum(s[0] for s in __assessment_scores.values())\n",
    "max_total = sum(s[1] for s in __assessment_scores.values())\n",
    "for exercise, (pts, max_pts) in __assessment_scores.items():\n",
    "    print(f\"  {exercise}: {pts}/{max_pts}\")\n",
    "    if exercise in __assessment_feedback:\n",
    "        for fb in __assessment_feedback[exercise]:\n",
    "            print(f\"    {fb}\")\n",
    "print(f\"\\nTotal: {total}/{max_total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}