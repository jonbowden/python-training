{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2394ad",
   "metadata": {},
   "source": [
    "# Module 3 — LLM Fundamentals (CodeVision Academy)\n",
    "\n",
    "## Overview\n",
    "This module introduces **Large Language Models (LLMs)** from an engineering and enterprise perspective.\n",
    "It is **code-first**, grounded in Python, and builds directly on:\n",
    "\n",
    "- **Module 1:** Python fundamentals (functions, JSON, notebooks)\n",
    "- **Module 2:** Data work with Pandas and visualisation\n",
    "\n",
    "You will learn how LLMs work, how to call them from Python, how they fail, and how to use them safely in regulated environments such as banking and financial services.\n",
    "\n",
    "### Supported LLM access methods (choose one)\n",
    "- **Local laptop LLM** — run a lightweight model using **Ollama** on your PC.\n",
    "- **Remote CodeVision LLM API** — an Ollama-compatible `/api/generate` endpoint provided by the course admin.\n",
    "\n",
    "Both options use the same request shape. Your code should work by changing **one** base URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6cbd51",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. Explain what an LLM is (and what it is not)\n",
    "2. Explain tokens, context windows, and training vs inference\n",
    "3. Call an LLM from Python via HTTP API (local or remote)\n",
    "4. Control determinism using temperature\n",
    "5. Force structured output (JSON) and validate it\n",
    "6. Recognise hallucinations and common failure modes\n",
    "7. Apply LLMs safely in a small data pipeline\n",
    "8. Explain why LLMs alone are insufficient for enterprise use, and why grounding (RAG) helps (Module 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d09d9",
   "metadata": {},
   "source": "## Setup — LLM Gateway Configuration\n\n### Choose Your LLM Access Method\n\nYou have **two options** for connecting to an LLM. Choose the one that suits your situation:\n\n---\n\n### Option A: Server-Side Gateway (Recommended for most students)\n\nUse the **centrally-managed LLM gateway** with an API key issued by your instructor. This requires no local setup.\n\n**Advantages:**\n- Works from any environment (Colab, local Jupyter, VS Code)\n- No local installation required\n- Consistent model availability\n- Connection over HTTPS\n\n**You will need:**\n1. `LLM_BASE_URL` — the gateway URL (provided by instructor)\n2. `LLM_API_KEY` — your API key (provided by instructor, for internal use only)\n\n---\n\n### Option B: Local LLM with Pinggy Tunnel\n\nRun a **local Ollama instance** on your laptop and expose it via a **pinggy tunnel** for HTTPS access.\n\n**Advantages:**\n- Full control over the model\n- Works offline (once model is downloaded)\n- No shared API key needed\n\n**Setup steps:**\n1. Install [Ollama](https://ollama.ai) on your laptop\n2. Pull a model: `ollama pull phi3:mini`\n3. Start Ollama: `ollama serve`\n4. Create a pinggy tunnel: `ssh -p 443 -R0:localhost:11434 a.pinggy.io`\n5. Use the pinggy HTTPS URL as your `LLM_BASE_URL`\n6. Leave `LLM_API_KEY` empty or set to `None`\n\n---\n\n### Configuration Cell\n\n**Uncomment the option you are using:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85339729",
   "metadata": {},
   "outputs": [],
   "source": "# ===== LLM GATEWAY CONFIGURATION =====\n# Uncomment ONE of the two options below\n\n# ------ OPTION A: Server-side gateway (with API key) ------\nLLM_BASE_URL = \"https://jbchat.jonbowden.com.ngrok.app\"  # Server gateway URL\nLLM_API_KEY = \"<provided-by-instructor>\"                  # API key (internal use only)\nENDPOINT_MODE = \"jbchat\"                                  # Use JBChat /chat/direct endpoint\n\n# ------ OPTION B: Local Ollama + Pinggy tunnel ------\n# LLM_BASE_URL = \"https://your-pinggy-url.a.pinggy.io\"   # Your pinggy tunnel URL\n# LLM_API_KEY = None                                      # No API key needed for local\n# ENDPOINT_MODE = \"ollama\"                                # Use Ollama /api/chat endpoint\n\n# ------ Model configuration ------\nDEFAULT_MODEL = \"phi3:mini\"      # Mandatory for this module\n# DEFAULT_MODEL = \"llama3.2:1b\"  # Optional comparison model"
  },
  {
   "cell_type": "markdown",
   "id": "e5baf971",
   "metadata": {},
   "source": "## Canonical LLM Caller — Single Source of Truth\n\nAll examples in this module use a single helper function: `call_llm()`. This function:\n\n- Supports **both modes**: JBChat gateway (`/chat/direct`) and direct Ollama (`/api/chat`)\n- Automatically selects the correct endpoint based on `ENDPOINT_MODE`\n- Includes bypass headers for tunnel services (ngrok, pinggy)\n- Returns the response text directly (not raw JSON)\n\n**Important:** All examples must use this function. No direct `requests.post()` calls elsewhere.\n\n| Mode | Endpoint | Use Case |\n|------|----------|----------|\n| `\"jbchat\"` | `/chat/direct` | Server-side gateway with API key |\n| `\"ollama\"` | `/api/chat` | Local Ollama via pinggy tunnel |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a3479",
   "metadata": {},
   "outputs": [],
   "source": "import requests\n\ndef call_llm(\n    prompt: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.0,\n    max_tokens: int = 256,\n    base_url: str = LLM_BASE_URL,\n    api_key: str | None = None,\n    timeout: tuple = (10, 120)\n) -> str:\n    \"\"\"\n    Canonical LLM call for Module 3.\n    Supports both JBChat gateway and direct Ollama endpoints.\n    \"\"\"\n    # Use module-level API key if not provided\n    if api_key is None:\n        api_key = LLM_API_KEY if LLM_API_KEY != \"<provided-by-instructor>\" else None\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n    }\n    \n    # Add bypass headers for tunnel services\n    headers[\"ngrok-skip-browser-warning\"] = \"true\"\n    headers[\"Bypass-Tunnel-Reminder\"] = \"true\"  # For pinggy\n    \n    if api_key:\n        headers[\"X-API-Key\"] = api_key\n\n    # Choose endpoint and payload format based on mode\n    if ENDPOINT_MODE == \"ollama\":\n        # Direct Ollama /api/chat endpoint\n        endpoint = f\"{base_url.rstrip('/')}/api/chat\"\n        payload = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"options\": {\"temperature\": temperature},\n            \"stream\": False\n        }\n    else:\n        # JBChat gateway /chat/direct endpoint\n        endpoint = f\"{base_url.rstrip('/')}/chat/direct\"\n        payload = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"stream\": False\n        }\n\n    resp = requests.post(\n        endpoint,\n        headers=headers,\n        json=payload,\n        timeout=timeout\n    )\n\n    resp.raise_for_status()\n    data = resp.json()\n\n    # Normalise output - extract content from message\n    return data[\"message\"][\"content\"]\n\n# Smoke test\ntry:\n    out = call_llm(\"In one sentence, define inflation for a banking audience.\", temperature=0.0)\n    print(out[:400])\nexcept Exception as e:\n    print(f\"Connection error (check LLM_BASE_URL and ENDPOINT_MODE): {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "6686c57a",
   "metadata": {},
   "source": [
    "# Section 3.1 — What is a Large Language Model?\n",
    "\n",
    "An LLM is best understood as a **next-token prediction engine**. It generates text that is statistically likely, not text that is guaranteed true.\n",
    "\n",
    "**Enterprise mindset:** treat LLM output as **untrusted** unless validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0f194",
   "metadata": {},
   "outputs": [],
   "source": "prompt = \"Complete: 'Interest rates are rising because'\"\nprint(call_llm(prompt, temperature=0.7)[:300])"
  },
  {
   "cell_type": "markdown",
   "id": "506f33ba",
   "metadata": {},
   "source": "# Section 3.2 — Tokens: How LLMs see text\n\nLLMs operate on **tokens** (subword pieces), not words. Tokenisation affects context limits and truncation.\n\n### Token Counting and Server-Side Processing\n\nKey concepts for enterprise use:\n\n- **Tokens are counted server-side** — the LLM gateway tracks usage\n- **`max_tokens` limits output**, not input — you control response length\n- **Long inputs increase:**\n  - Latency (more to process)\n  - Truncation risk (may hit context limit)\n  - Timeout probability (especially with small models)\n- **Small models exaggerate these effects** — useful for learning, but plan for larger models in production\n\nPractical implication: keep prompts concise and plan for chunking on long documents."
  },
  {
   "cell_type": "markdown",
   "id": "d77c6027",
   "metadata": {},
   "source": [
    "# Section 3.3 — Training vs inference\n",
    "\n",
    "- **Training**: offline learning of model parameters from huge datasets.\n",
    "- **Inference**: runtime generation when you call the model endpoint.\n",
    "\n",
    "This module focuses on inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfc117",
   "metadata": {},
   "outputs": [],
   "source": "resp = call_llm(\"Explain training vs inference in 2 bullet points.\", temperature=0.0)\nprint(resp)"
  },
  {
   "cell_type": "markdown",
   "id": "00392b1f",
   "metadata": {},
   "source": [
    "# Section 3.4 — LLMs as services (APIs)\n",
    "\n",
    "Treat the LLM like any other service: send JSON request, receive JSON response. This builds on your JSON and requests skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0270a08a",
   "metadata": {},
   "outputs": [],
   "source": "resp = call_llm(\"Say hello.\")\nprint(f\"Response type: {type(resp)}\")\nprint(f\"Response: {resp}\")"
  },
  {
   "cell_type": "markdown",
   "id": "fc7d183d",
   "metadata": {},
   "source": [
    "# Section 3.5 — Prompt structure: role, task, constraints\n",
    "\n",
    "With single-prompt endpoints, simulate roles by placing behaviour rules first, task second, constraints last.\n",
    "\n",
    "This reduces ambiguity and improves reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9618040",
   "metadata": {},
   "outputs": [],
   "source": "system = \"You are a cautious banking analyst. Do not speculate. If unsure, say 'Insufficient information'.\"\ntask = \"Summarise for an executive: FX volatility increased due to rate differentials.\"\nconstraints = \"Return exactly 2 bullet points. Max 20 words each.\"\nprompt = f\"SYSTEM:\\n{system}\\n\\nTASK:\\n{task}\\n\\nCONSTRAINTS:\\n{constraints}\"\nprint(call_llm(prompt, temperature=0.0))"
  },
  {
   "cell_type": "markdown",
   "id": "6c588100",
   "metadata": {},
   "source": [
    "# Section 3.6 — Temperature and determinism\n",
    "\n",
    "Temperature controls randomness. Low temperature (0.0–0.2) is preferred in regulated workflows for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcae6be",
   "metadata": {},
   "outputs": [],
   "source": "prompt = \"Explain what a context window is in 2 sentences.\"\nlow = call_llm(prompt, temperature=0.0)\nhigh = call_llm(prompt, temperature=0.8)\nprint(\"Temp 0.0:\\n\", low)\nprint(\"\\nTemp 0.8:\\n\", high)"
  },
  {
   "cell_type": "markdown",
   "id": "e73b8460",
   "metadata": {},
   "source": "# Section 3.7 — Hallucinations (confident but wrong)\n\nHallucinations occur because the model optimises for **plausible text** rather than **verified truth**. The model will confidently generate answers even when the question refers to something that does not exist.\n\n**Teaching goal:** *Hallucination is not random error — it is plausible continuation beating truthful uncertainty.*\n\nNever treat confident language as evidence. Always verify claims against trusted sources."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1405bcc",
   "metadata": {},
   "outputs": [],
   "source": "# Hallucination demonstration: asking about a paper that does not exist\nprompt = \"\"\"Explain the key ideas from the 2019 paper\n\"Temporal Diffusion Graph Transformers for Quantum Finance\"\nby Liu and Henderson, published at NeurIPS.\"\"\"\n\nresponse = call_llm(prompt, temperature=0.0, max_tokens=512)\nprint(\"LLM Response:\")\nprint(response)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ANALYSIS: Why this is a hallucination\")\nprint(\"=\"*60)\nprint(\"\"\"\nThis paper DOES NOT EXIST. The model's response demonstrates classic hallucination patterns:\n\n1. PAPER SUBSTITUTION: The model invents plausible-sounding content based on\n   keywords (transformers, finance, quantum). It may cite real papers or\n   concepts that are unrelated.\n\n2. CONFIDENT SPECULATION: Watch for phrases like \"likely\", \"would probably\",\n   \"typically involves\" — these mask uncertainty as knowledge.\n\n3. GENERIC ML BOILERPLATE: The response uses standard ML vocabulary\n   (attention mechanisms, embeddings, architectures) that sounds authoritative\n   but is not grounded in any real paper.\n\n4. DOMAIN DRIFT: The model may conflate \"quantum finance\" (a real niche field)\n   with generic finance ML, producing plausible but wrong explanations.\n\n5. HEDGING ADMISSION: Sometimes the model adds \"if such a paper exists\" or\n   similar — but still provides fabricated details anyway.\n\nKEY LESSON: Confident language ≠ truthful content. Always verify against\nauthoritative sources (actual paper, official database, domain expert).\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "c3e20418",
   "metadata": {},
   "source": "# Section 3.11 — Defensive parsing and validation\n\nModels may return invalid JSON for several reasons:\n- **Markdown wrapping** — response wrapped in ` ```json ... ``` ` blocks\n- **Trailing text** — explanatory text after the JSON\n- **Malformed structure** — missing quotes, trailing commas, etc.\n- **Empty response** — timeout or model failure\n\nThe `safe_json_loads()` function below handles these cases:\n1. Strips markdown code block wrappers\n2. Attempts JSON parsing\n3. Returns a tuple: `(success, result_or_error)`\n\n**Enterprise pattern:** Always wrap JSON parsing in try/except and have a fallback strategy."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d3f26",
   "metadata": {},
   "outputs": [],
   "source": "# Moderate example: a reasonably long input (not extreme)\n# Note: We use a moderate size to avoid destabilising small models\npolicy_text = (\"This is a paragraph from a banking policy document covering risk management. \" * 50)\nprompt = f\"Summarise in 3 bullets:\\n{policy_text}\"\n\ntry:\n    response = call_llm(prompt, temperature=0.0, max_tokens=256)\n    print(\"Summary:\")\n    print(response[:600])\nexcept Exception as e:\n    print(f\"Request failed (expected for very long inputs): {e}\")\n    print(\"In production, you would chunk the input or use a larger model.\")"
  },
  {
   "cell_type": "markdown",
   "id": "faa4a3ff",
   "metadata": {},
   "source": [
    "# Section 3.9 — Prompt hygiene: common mistakes and fixes\n",
    "\n",
    "Avoid vague asks, missing constraints, and multi-task prompts. Prefer clear audience, format, and uncertainty policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8704b5",
   "metadata": {},
   "outputs": [],
   "source": "bad = \"Tell me about interest rates.\"\ngood = \"Explain interest rates to a new bank analyst in 3 bullets, <= 18 words each. No speculation.\"\nprint(\"BAD:\\n\", call_llm(bad, temperature=0.0))\nprint(\"\\nGOOD:\\n\", call_llm(good, temperature=0.0))"
  },
  {
   "cell_type": "markdown",
   "id": "6190194c",
   "metadata": {},
   "source": "# Section 3.10 — Structured output: why JSON matters\n\nJSON output enables deterministic parsing, validation, and automation. This builds directly on Module 1.\n\n### Common Issue: Markdown-Wrapped JSON\n\nLLMs often return JSON wrapped in markdown code blocks:\n\n```\n```json\n{\"key\": \"value\"}\n```\n```\n\nThis causes `json.loads()` to fail! You must **strip the markdown wrapper** before parsing.\n\nThe `strip_markdown_json()` helper function below handles this:\n1. Detects if the response starts with ` ```json ` or ` ``` `\n2. Extracts the content between the backticks\n3. Returns clean JSON ready for parsing\n\n**Always use this pattern when parsing LLM JSON output.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bec09",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport re\n\ndef strip_markdown_json(s: str) -> str:\n    \"\"\"\n    Strip markdown code block wrappers from LLM JSON output.\n    \n    LLMs often return JSON wrapped in markdown:\n        ```json\n        {\"key\": \"value\"}\n        ```\n    \n    This function extracts the raw JSON for parsing.\n    \"\"\"\n    s = s.strip()\n    # Pattern matches ```json or ``` at start, and ``` at end\n    pattern = r'^```(?:json)?\\s*\\n?(.*?)\\n?```$'\n    match = re.match(pattern, s, re.DOTALL | re.IGNORECASE)\n    if match:\n        return match.group(1).strip()\n    return s\n\nprompt = (\n\"Return ONLY valid JSON with keys: summary (string), risks (array of exactly 3 strings). \"\n\"No extra text. Use double quotes. \"\n\"Text: Banks face credit risk, market risk, and operational risk.\"\n)\nraw = call_llm(prompt, temperature=0.0)\nprint(\"Raw response:\")\nprint(raw)\n\n# IMPORTANT: LLMs often wrap JSON in markdown code blocks - strip before parsing\ncleaned = strip_markdown_json(raw)\nprint(\"\\nCleaned for parsing:\")\nprint(cleaned)\n\ntry:\n    data = json.loads(cleaned)\n    print(\"\\nParsed successfully:\")\n    print(data)\nexcept json.JSONDecodeError as e:\n    print(f\"\\nInvalid JSON - do not proceed: {e}\")\n    print(\"In production, you would retry or fail gracefully here.\")"
  },
  {
   "cell_type": "markdown",
   "id": "c87d31e8",
   "metadata": {},
   "source": [
    "# Section 3.11 — Defensive parsing and validation\n",
    "\n",
    "Models sometimes return invalid JSON. Handle this safely: parse, validate, retry or fail clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0691996",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport re\n\ndef strip_markdown_json(s: str) -> str:\n    \"\"\"\n    Strip markdown code block wrappers from LLM JSON output.\n    \n    LLMs often return JSON wrapped in markdown:\n        ```json\n        {\"key\": \"value\"}\n        ```\n    \n    This function extracts the raw JSON for parsing.\n    \"\"\"\n    s = s.strip()\n    # Pattern matches ```json or ``` at start, and ``` at end\n    pattern = r'^```(?:json)?\\s*\\n?(.*?)\\n?```$'\n    match = re.match(pattern, s, re.DOTALL | re.IGNORECASE)\n    if match:\n        return match.group(1).strip()\n    return s\n\ndef safe_json_loads(s: str) -> tuple:\n    \"\"\"\n    Attempt to parse JSON safely, handling markdown-wrapped responses.\n    Returns (success, result_or_error).\n    \"\"\"\n    # First, strip any markdown code block wrappers\n    cleaned = strip_markdown_json(s)\n    try:\n        return True, json.loads(cleaned)\n    except Exception as e:\n        return False, f\"{type(e).__name__}: {e}\"\n\n# Example: LLM may return JSON wrapped in markdown code blocks\nraw = call_llm('Return JSON only: {\"a\": 1}', temperature=0.0)\nprint(f\"Raw response: {raw!r}\")\n\nok, parsed = safe_json_loads(raw)\nprint(f\"Parse successful: {ok}\")\nif ok:\n    print(f\"Parsed data: {parsed}\")\nelse:\n    print(f\"Error: {parsed}\")"
  },
  {
   "cell_type": "markdown",
   "id": "6962a733",
   "metadata": {},
   "source": [
    "# Section 3.12 — Text validators: length, bullets, vocabulary\n",
    "\n",
    "Not all tasks need JSON. You can validate text using deterministic rules like bullet count and max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c894671",
   "metadata": {},
   "outputs": [],
   "source": "text = call_llm(\"Return exactly 3 bullet points about liquidity risk.\", temperature=0.0)\nbullets = [ln for ln in text.splitlines() if ln.strip().startswith((\"-\", \"*\"))]\nprint(\"Bullet count:\", len(bullets))\nprint(text)"
  },
  {
   "cell_type": "markdown",
   "id": "3c39bd7b",
   "metadata": {},
   "source": [
    "# Section 3.13 — LLMs inside a Pandas pipeline\n",
    "\n",
    "LLMs can augment data pipelines by generating summaries or tags. Start small and validate outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb0595",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\ndf = pd.DataFrame({\n    \"id\": [1,2,3],\n    \"text\": [\n        \"Credit risk is the possibility of loss from borrower default.\",\n        \"Market risk comes from adverse movements in interest rates and FX.\",\n        \"Operational risk arises from process, people, or system failures.\"\n    ]\n})\ndef summarise_row(t: str) -> str:\n    prompt = f\"Summarise in 10 words or fewer: {t}\"\n    return call_llm(prompt, temperature=0.0).strip()\ndf[\"summary\"] = df[\"text\"].apply(summarise_row)\ndf"
  },
  {
   "cell_type": "markdown",
   "id": "52307074",
   "metadata": {},
   "source": [
    "# Section 3.14 — Cost/latency mindset: caching\n",
    "\n",
    "LLM calls are slow compared to normal functions. Use caching for repeated prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e67697",
   "metadata": {},
   "outputs": [],
   "source": "_cache = {}\ndef cached_llm(prompt: str, temperature: float = 0.0) -> str:\n    key = (prompt, temperature, DEFAULT_MODEL, LLM_BASE_URL)\n    if key in _cache:\n        return _cache[key]\n    out = call_llm(prompt, temperature=temperature).strip()\n    _cache[key] = out\n    return out\np = \"Summarise: Banks face credit and market risk.\"\nprint(cached_llm(p, 0.0))\nprint(cached_llm(p, 0.0))"
  },
  {
   "cell_type": "markdown",
   "id": "20793697",
   "metadata": {},
   "source": [
    "# Section 3.15 — Local vs remote endpoint trade-offs\n",
    "\n",
    "Local: simple, private, predictable. Remote: centrally managed, potentially faster, requires network/access control. Your code should work for both by switching LLM_BASE_URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17109616",
   "metadata": {},
   "source": [
    "# Section 3.16 — Enterprise constraints: auditability and compliance\n",
    "\n",
    "Log prompts (or hashes), parameters, model, and output metadata for auditability. Avoid sending sensitive data to unapproved endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459bcfd",
   "metadata": {},
   "outputs": [],
   "source": "import hashlib, time\ndef audit_meta(prompt: str, response_text: str, model: str, temperature: float) -> dict:\n    return {\n        \"ts\": time.time(),\n        \"model\": model,\n        \"temperature\": temperature,\n        \"prompt_sha256\": hashlib.sha256(prompt.encode()).hexdigest(),\n        \"response_sha256\": hashlib.sha256(response_text.encode()).hexdigest(),\n        \"response_len\": len(response_text),\n    }\np = \"Summarise operational risk in 12 words.\"\nresp = call_llm(p, temperature=0.0)\nmeta = audit_meta(p, resp, DEFAULT_MODEL, 0.0)\nmeta"
  },
  {
   "cell_type": "markdown",
   "id": "z92y2eeqwdc",
   "source": "# Section 3.16b — API Gateway Security: Preventing Misuse\n\nWhen exposing LLM services via an API gateway (like ngrok), security is critical. Unsecured endpoints can be:\n\n- **Abused for free compute** — attackers use your LLM for their own purposes\n- **Used for prompt injection attacks** — malicious prompts extracting sensitive data\n- **Overwhelmed by DoS** — excessive requests crashing your service\n- **Scraped for model outputs** — competitors harvesting your model's responses\n\n### Security Measures for LLM Gateways\n\n| Layer | Measure | Purpose |\n|-------|---------|---------|\n| **Authentication** | API keys (`X-API-Key` header) | Identify and authorise callers |\n| **Rate Limiting** | Requests per minute/hour per key | Prevent abuse and DoS |\n| **Input Validation** | Max prompt length, blocked patterns | Prevent injection attacks |\n| **Output Filtering** | Sanitise responses, remove PII | Prevent data leakage |\n| **Logging & Monitoring** | Track usage per key, alert on anomalies | Detect and respond to abuse |\n| **Token Quotas** | Max tokens per key per day | Control costs and fair usage |\n\n### Implementation Approaches\n\n1. **API Gateway Layer** (AWS API Gateway, Kong, nginx):\n   - Rate limiting and throttling\n   - API key validation\n   - Request/response logging\n\n2. **Application Layer**:\n   - Input sanitisation before LLM call\n   - Output filtering after LLM response\n   - User-specific quotas in database\n\n3. **Network Layer**:\n   - IP allowlisting for internal use\n   - TLS/HTTPS only (ngrok provides this)\n   - VPN for sensitive deployments\n\n### Enterprise Best Practice\n\nIn production, **never expose raw LLM endpoints publicly**. Always:\n\n- Wrap in an authenticated API layer\n- Log all requests with user identity\n- Set hard limits on usage per user/key\n- Monitor for anomalous patterns (unusual prompts, high volume)\n- Have an incident response plan for detected abuse",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "f588d412",
   "metadata": {},
   "source": [
    "# Section 3.17 — Evaluation without using another LLM\n",
    "\n",
    "Prefer deterministic checks: schema validation, key checks, length constraints, bullet counts. Avoid 'LLM judging LLM' as your only control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362b0ee",
   "metadata": {},
   "source": [
    "# Section 3.18 — Safety patterns: uncertainty and fallbacks\n",
    "\n",
    "Include an uncertainty policy: if unsure, say 'Insufficient information'. Build fallbacks when validation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b401c9",
   "metadata": {},
   "outputs": [],
   "source": "system = \"If you are unsure, respond exactly: Insufficient information. Do not guess.\"\ntask = \"What is the exact USD/GBP rate at 09:31 UTC yesterday?\"\nprompt = f\"{system}\\n\\n{task}\"\nprint(call_llm(prompt, temperature=0.0))"
  },
  {
   "cell_type": "markdown",
   "id": "8feb7962",
   "metadata": {},
   "source": [
    "# Section 3.19 — Why LLMs alone are not enough\n",
    "\n",
    "LLMs have hallucinations, context limits, and no grounding in your internal data by default. This motivates grounding and retrieval techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db0ee7",
   "metadata": {},
   "source": [
    "# Section 3.20 — Preparing for Module 4 (Grounding / RAG)\n",
    "\n",
    "Mental model: LLM = language engine; RAG = evidence + memory. RAG reduces hallucinations by supplying trusted context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc72488",
   "metadata": {},
   "source": [
    "## Practice exercises (ungraded)\n",
    "1. Force JSON output for a classification task and parse it.\n",
    "2. Demonstrate one hallucination and explain why it happened.\n",
    "3. Enrich a small DataFrame with LLM-generated summaries and add a cache.\n",
    "4. Add a validator enforcing: exactly 3 bullets and <= 20 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e69a1",
   "metadata": {},
   "source": [
    "## Module summary\n",
    "- LLMs generate **probabilistic text**, not guaranteed truth.\n",
    "- Treat outputs as **untrusted** unless validated.\n",
    "- Use **low temperature** for consistency and auditability.\n",
    "- Prefer **structured outputs (JSON)** for automation.\n",
    "- Design for **failures, retries, and fallbacks**."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}