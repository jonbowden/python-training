{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 3 - LLM Fundamentals Assessment (Instructor Version)\n",
    "\n",
    "**INSTRUCTOR / GRADING TEMPLATE - Spark LLM Edition**\n",
    "\n",
    "This notebook uses the Spark-hosted LLM for grading, with rubric-aligned scoring:\n",
    "\n",
    "| Rubric Area | Weight | Method |\n",
    "|------------|--------|--------|\n",
    "| Exercise structure & types | 40% | Code inspection + runtime |\n",
    "| JSON robustness | 30% | Injected malformed responses |\n",
    "| Live LLM tolerance | 30% | Spark API calls |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-scoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN: SCORING SETUP ===\n",
    "__assessment_scores = {}\n",
    "__assessment_feedback = {}\n",
    "\n",
    "def record_score(exercise, points, max_points, feedback=None):\n",
    "    __assessment_scores[exercise] = (points, max_points)\n",
    "    if feedback:\n",
    "        __assessment_feedback[exercise] = feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-config",
   "metadata": {},
   "outputs": [],
   "source": "# === GRADING ENVIRONMENT CONFIG ===\nimport os\nimport requests\nimport json\nimport re\n\n# Spark LLM via ngrok gateway - uses /chat/direct with API key\nSPARK_BASE_URL = os.environ.get('SPARK_BASE_URL', 'https://jbchat.jonbowden.com.ngrok.app')\nSPARK_API_KEY = os.environ.get('SPARK_API_KEY')  # Set in GitHub Actions secrets\nLLM_BASE_URL = SPARK_BASE_URL  # Alias for student code compatibility\nLLM_API_KEY = None  # Student code uses local Ollama\nDEFAULT_MODEL = \"phi3:mini\"\n\n_spark_available = None\n\ndef check_spark_available():\n    \"\"\"Check if Spark LLM is reachable via /chat/direct.\"\"\"\n    global _spark_available\n    if _spark_available is not None:\n        return _spark_available\n    if not SPARK_API_KEY:\n        print(\"DEBUG: No SPARK_API_KEY set, using mock\")\n        _spark_available = False\n        return False\n    try:\n        # Test with a minimal request\n        payload = {\"model\": DEFAULT_MODEL, \"messages\": [{\"role\": \"user\", \"content\": \"test\"}], \"stream\": False}\n        headers = {\n            'Content-Type': 'application/json',\n            'X-API-Key': SPARK_API_KEY,\n            'ngrok-skip-browser-warning': 'true'\n        }\n        r = requests.post(f\"{SPARK_BASE_URL}/chat/direct\", json=payload, headers=headers, timeout=15)\n        _spark_available = r.status_code == 200\n        if not _spark_available:\n            print(f\"DEBUG: Spark check failed with status {r.status_code}\")\n    except Exception as e:\n        print(f\"DEBUG: Spark check failed: {e}\")\n        _spark_available = False\n    return _spark_available\n\ndef call_spark_llm(prompt: str, temperature: float = 0.0) -> str:\n    \"\"\"Call Spark LLM API via /chat/direct endpoint.\"\"\"\n    payload = {\n        \"model\": DEFAULT_MODEL,\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"temperature\": temperature,\n        \"stream\": False\n    }\n    headers = {\n        'Content-Type': 'application/json',\n        'X-API-Key': SPARK_API_KEY,\n        'ngrok-skip-browser-warning': 'true',\n        'Bypass-Tunnel-Reminder': 'true'\n    }\n    r = requests.post(f\"{SPARK_BASE_URL}/chat/direct\", json=payload, headers=headers, timeout=60)\n    r.raise_for_status()\n    return r.json()[\"message\"][\"content\"]\n\n# === MOCK LLM FALLBACK ===\n# Used only when Spark gateway is unreachable or no API key\n_original_post = requests.post\n_mock_call_count = 0\n\ndef _mock_post(url, **kwargs):\n    \"\"\"Mock requests.post for LLM endpoints when Spark unavailable.\"\"\"\n    global _mock_call_count\n    _mock_call_count += 1\n    \n    if '/api/chat' in url or '/chat/direct' in url:\n        payload = kwargs.get('json', {})\n        messages = payload.get('messages', [])\n        prompt = messages[0].get('content', '') if messages else ''\n        temperature = payload.get('temperature', 0.0)\n        \n        if 'json' in prompt.lower() or '{' in prompt:\n            if _mock_call_count % 2 == 0:\n                response_text = '```json\\n{\"test\": 1, \"value\": 42}\\n```'\n            else:\n                response_text = '{\"test\": 1, \"value\": 42}'\n        elif 'hello' in prompt.lower():\n            response_text = \"Hello!\"\n        elif 'color' in prompt.lower() or 'fruit' in prompt.lower():\n            if temperature > 0.5:\n                response_text = \"Red, Blue, Green (high temp response)\"\n            else:\n                response_text = \"Red, Blue, Green\"\n        else:\n            response_text = f\"Mock response for: {prompt[:50]}...\"\n        \n        class MockResponse:\n            status_code = 200\n            def raise_for_status(self): pass\n            def json(self): return {\"message\": {\"content\": response_text}}\n        \n        return MockResponse()\n    \n    return _original_post(url, **kwargs)\n\n# Check Spark availability and set mode\n_grading_mode = 'spark' if check_spark_available() else 'mock'\nif _grading_mode == 'mock':\n    requests.post = _mock_post\nprint(f\"DEBUG: GRADING_MODE={_grading_mode.upper()}, SPARK_URL={SPARK_BASE_URL}\")"
  },
  {
   "cell_type": "markdown",
   "id": "ex1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 - Basic LLM Caller (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN TEST: Exercise 1 ===\n",
    "points = 0\n",
    "feedback = []\n",
    "\n",
    "# Test 1: Function exists\n",
    "try:\n",
    "    assert 'call_llm' in dir() or 'call_llm' in globals(), \"Function 'call_llm' not defined\"\n",
    "    assert callable(call_llm), \"'call_llm' should be a function\"\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Function 'call_llm' defined\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"✗ {e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Error: {type(e).__name__}\")\n",
    "\n",
    "# Test 2: Returns string\n",
    "try:\n",
    "    result = call_llm(\"Say hello in one word.\")\n",
    "    assert isinstance(result, str), f\"Should return str, got {type(result).__name__}\"\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Function returns a string\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"✗ {e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Error calling function: {type(e).__name__}\")\n",
    "\n",
    "# Test 3: Non-empty response\n",
    "try:\n",
    "    result = call_llm(\"Say hello in one word.\")\n",
    "    assert len(result) > 0, \"Response should not be empty\"\n",
    "    points += 1\n",
    "    feedback.append(f\"✓ Response contains text ({len(result)} chars)\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"✗ {e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Error: {type(e).__name__}\")\n",
    "\n",
    "# Test 4: Temperature parameter\n",
    "try:\n",
    "    result = call_llm(\"Say hello.\", temperature=0.5)\n",
    "    assert isinstance(result, str), \"Should work with temperature parameter\"\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Temperature parameter works\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"✗ {e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Temperature param error: {type(e).__name__}\")\n",
    "\n",
    "record_score('Exercise 1', points, 4, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 - Extract Response Text (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN TEST: Exercise 2 ===\n",
    "points = 0\n",
    "feedback = []\n",
    "\n",
    "try:\n",
    "    assert 'get_response_text' in dir() or 'get_response_text' in globals()\n",
    "    assert callable(get_response_text)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Function 'get_response_text' defined\")\n",
    "except:\n",
    "    feedback.append(\"✗ Function 'get_response_text' not defined\")\n",
    "\n",
    "try:\n",
    "    result = get_response_text(\"Say hello\")\n",
    "    assert isinstance(result, str)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Function returns a string\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Error: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = get_response_text(\"Say hello\")\n",
    "    assert len(result) > 0\n",
    "    points += 1\n",
    "    feedback.append(f\"✓ Returns non-empty text ({len(result)} chars)\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Error: {type(e).__name__}\")\n",
    "\n",
    "record_score('Exercise 2', points, 3, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-header",
   "metadata": {},
   "source": [
    "## Exercise 3 - JSON Output Parser (4 points)\n",
    "\n",
    "**Rubric focus: JSON Robustness (30% of module)**\n",
    "\n",
    "Tests student's ability to handle malformed LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN TEST: Exercise 3 - JSON Robustness ===\n",
    "points = 0\n",
    "feedback = []\n",
    "\n",
    "# Test 1: Function exists and returns tuple\n",
    "try:\n",
    "    assert 'parse_json_response' in dir() or 'parse_json_response' in globals()\n",
    "    assert callable(parse_json_response)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Function 'parse_json_response' defined\")\n",
    "except:\n",
    "    feedback.append(\"✗ Function 'parse_json_response' not defined\")\n",
    "\n",
    "# Test 2: Returns correct tuple format\n",
    "try:\n",
    "    result = parse_json_response('Return JSON: {\"a\": 1}')\n",
    "    assert isinstance(result, tuple) and len(result) == 2\n",
    "    assert isinstance(result[0], bool)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Returns (bool, result) tuple\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Wrong return format: {type(e).__name__}\")\n",
    "\n",
    "# Test 3: Handles markdown-wrapped JSON (critical robustness test)\n",
    "MALFORMED_TESTS = [\n",
    "    ('```json\\n{\"test\": 1}\\n```', 'markdown-wrapped'),\n",
    "    ('{\"test\": 1}', 'clean JSON'),\n",
    "]\n",
    "\n",
    "passed_malformed = 0\n",
    "for test_input, desc in MALFORMED_TESTS:\n",
    "    try:\n",
    "        # Temporarily override get_response_text to return test input\n",
    "        _orig_get = get_response_text if 'get_response_text' in dir() else None\n",
    "        def mock_get(p): return test_input\n",
    "        globals()['get_response_text'] = mock_get\n",
    "        \n",
    "        success, result = parse_json_response('test')\n",
    "        if success and isinstance(result, dict):\n",
    "            passed_malformed += 1\n",
    "        \n",
    "        if _orig_get:\n",
    "            globals()['get_response_text'] = _orig_get\n",
    "    except:\n",
    "        if _orig_get:\n",
    "            globals()['get_response_text'] = _orig_get\n",
    "\n",
    "if passed_malformed >= 1:\n",
    "    points += 1\n",
    "    feedback.append(f\"✓ Handles {passed_malformed}/2 JSON formats\")\n",
    "else:\n",
    "    feedback.append(\"✗ Failed to parse JSON responses\")\n",
    "\n",
    "# Test 4: Live LLM JSON parsing\n",
    "try:\n",
    "    success, result = parse_json_response('Return ONLY: {\"value\": 42}')\n",
    "    if success:\n",
    "        points += 1\n",
    "        feedback.append(\"✓ Parses live LLM JSON response\")\n",
    "    else:\n",
    "        feedback.append(f\"✗ Live JSON parse failed: {result}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Live test error: {type(e).__name__}\")\n",
    "\n",
    "record_score('Exercise 3', points, 4, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-header",
   "metadata": {},
   "source": [
    "## Exercise 4 - Temperature Comparison (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN TEST: Exercise 4 ===\n",
    "points = 0\n",
    "feedback = []\n",
    "\n",
    "try:\n",
    "    assert 'compare_temperatures' in dir() or 'compare_temperatures' in globals()\n",
    "    assert callable(compare_temperatures)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Function 'compare_temperatures' defined\")\n",
    "except:\n",
    "    feedback.append(\"✗ Function not defined\")\n",
    "\n",
    "try:\n",
    "    result = compare_temperatures(\"List 3 colors.\")\n",
    "    assert isinstance(result, dict)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Returns a dictionary\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Error: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = compare_temperatures(\"List 3 colors.\")\n",
    "    assert 'low_temp' in result and 'high_temp' in result and 'are_identical' in result\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Has all required keys\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Missing keys: {e}\")\n",
    "\n",
    "try:\n",
    "    result = compare_temperatures(\"List 3 colors.\")\n",
    "    assert isinstance(result['are_identical'], bool)\n",
    "    assert isinstance(result['low_temp'], str)\n",
    "    assert isinstance(result['high_temp'], str)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ All values have correct types\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Type error: {e}\")\n",
    "\n",
    "record_score('Exercise 4', points, 4, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5-header",
   "metadata": {},
   "source": [
    "## Exercise 5 - Structured Prompt Builder (5 points)\n",
    "\n",
    "**Rubric focus: Prompt Discipline (35% of module)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN TEST: Exercise 5 - Prompt Discipline ===\n",
    "points = 0\n",
    "feedback = []\n",
    "\n",
    "try:\n",
    "    assert 'build_structured_prompt' in dir() or 'build_structured_prompt' in globals()\n",
    "    assert callable(build_structured_prompt)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Function 'build_structured_prompt' defined\")\n",
    "except:\n",
    "    feedback.append(\"✗ Function not defined\")\n",
    "\n",
    "try:\n",
    "    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Short\"])\n",
    "    assert isinstance(result, str)\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Returns a string\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"✗ Error: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Short\"])\n",
    "    assert 'SYSTEM:' in result and 'TASK:' in result and 'CONSTRAINTS:' in result\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Contains all section labels\")\n",
    "except:\n",
    "    feedback.append(\"✗ Missing section labels\")\n",
    "\n",
    "try:\n",
    "    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Short\", \"Clear\"])\n",
    "    assert '- Short' in result or '- short' in result.lower()\n",
    "    assert '- Clear' in result or '- clear' in result.lower()\n",
    "    points += 1\n",
    "    feedback.append(\"✓ Constraints formatted with '- ' prefix\")\n",
    "except:\n",
    "    feedback.append(\"✗ Constraints not properly formatted\")\n",
    "\n",
    "try:\n",
    "    result = build_structured_prompt(\"Test sys\", \"Test task\", [\"C1\"])\n",
    "    assert 'Test sys' in result and 'Test task' in result\n",
    "    points += 1\n",
    "    feedback.append(\"✓ All inputs included in output\")\n",
    "except:\n",
    "    feedback.append(\"✗ Inputs not in output\")\n",
    "\n",
    "record_score('Exercise 5', points, 5, feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-results",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN: WRITE RESULTS ===\nimport json\nimport datetime\n\nresult = {\n    'scores': __assessment_scores,\n    'feedback': __assessment_feedback,\n    'timestamp': datetime.datetime.now().isoformat(),\n    'grading_mode': _grading_mode\n}\n\nwith open('assessment_result.json', 'w') as f:\n    json.dump(result, f, indent=2)\n\nprint(\"Assessment Results:\")\nprint(f\"Grading mode: {_grading_mode.upper()}\")\ntotal = sum(s[0] for s in __assessment_scores.values())\nmax_total = sum(s[1] for s in __assessment_scores.values())\nfor exercise, (pts, max_pts) in __assessment_scores.items():\n    print(f\"  {exercise}: {pts}/{max_pts}\")\n    if exercise in __assessment_feedback:\n        for fb in __assessment_feedback[exercise]:\n            print(f\"    {fb}\")\nprint(f\"\\nTotal: {total}/{max_total}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}