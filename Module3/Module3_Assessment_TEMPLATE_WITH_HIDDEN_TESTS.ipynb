{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 3 - LLM Fundamentals Assessment (Instructor Version)\n",
    "\n",
    "**INSTRUCTOR / GRADING TEMPLATE**\n",
    "\n",
    "This notebook contains **hidden assessment logic** and must NOT be shared with students.\n",
    "\n",
    "Purpose:\n",
    "- Inject student code programmatically\n",
    "- Run automated tests\n",
    "- Produce authoritative scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-scoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN: SCORING SETUP ===\n",
    "__assessment_scores = {}\n",
    "__assessment_feedback = {}\n",
    "\n",
    "def record_score(exercise, points, max_points, feedback=None):\n",
    "    __assessment_scores[exercise] = (points, max_points)\n",
    "    if feedback:\n",
    "        __assessment_feedback[exercise] = feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-config",
   "metadata": {},
   "outputs": [],
   "source": "# === GRADING ENVIRONMENT CONFIG ===\n# For grading, use local Ollama (no API key needed)\nLLM_BASE_URL = \"http://localhost:11434\"\nLLM_API_KEY = None  # Local testing - no API key\nENDPOINT_MODE = \"ollama\"  # Use Ollama /api/chat endpoint\nDEFAULT_MODEL = \"phi3:mini\"\n\nimport requests\nimport json\nimport re"
  },
  {
   "cell_type": "markdown",
   "id": "ex1-header",
   "metadata": {},
   "source": [
    "## Exercise 1 - Basic LLM Caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 1 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'call_llm' in dir() or 'call_llm' in globals(), \"Function 'call_llm' not defined\"\n    assert callable(call_llm), \"'call_llm' should be a function\"\n    points += 1\n    feedback.append(\"Function 'call_llm' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"{e}\")\nexcept Exception as e:\n    feedback.append(f\"Error checking function: {type(e).__name__}\")\n\ntry:\n    result = call_llm(\"Say hello in one word.\")\n    assert isinstance(result, str), f\"Should return str, got {type(result).__name__}\"\n    points += 1\n    feedback.append(\"Function returns a string\")\nexcept AssertionError as e:\n    feedback.append(f\"{e}\")\nexcept Exception as e:\n    feedback.append(f\"Error calling function: {type(e).__name__}: {e}\")\n\ntry:\n    result = call_llm(\"Say hello in one word.\")\n    assert len(result) > 0, \"Response should not be empty\"\n    points += 1\n    feedback.append(f\"Response contains text ({len(result)} chars)\")\nexcept AssertionError as e:\n    feedback.append(f\"{e}\")\nexcept Exception as e:\n    feedback.append(f\"Error checking response: {type(e).__name__}\")\n\ntry:\n    # Test temperature parameter\n    result = call_llm(\"Say hello in one word.\", temperature=0.5)\n    assert isinstance(result, str), \"Should work with temperature parameter\"\n    points += 1\n    feedback.append(\"Temperature parameter works\")\nexcept AssertionError as e:\n    feedback.append(f\"{e}\")\nexcept Exception as e:\n    feedback.append(f\"Error with temperature param: {type(e).__name__}\")\n\nrecord_score('Exercise 1', points, 4, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "ex2-header",
   "metadata": {},
   "source": [
    "## Exercise 2 - Extract Response Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN TEST: Exercise 2 ===\n",
    "points = 0\n",
    "feedback = []\n",
    "\n",
    "try:\n",
    "    assert 'get_response_text' in dir() or 'get_response_text' in globals(), \"Function 'get_response_text' not defined\"\n",
    "    assert callable(get_response_text), \"'get_response_text' should be a function\"\n",
    "    points += 1\n",
    "    feedback.append(\"Function 'get_response_text' defined\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking function: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = get_response_text(\"Say hello\")\n",
    "    assert isinstance(result, str), f\"Should return str, got {type(result).__name__}\"\n",
    "    points += 1\n",
    "    feedback.append(\"Function returns a string\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error calling function: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = get_response_text(\"Say hello\")\n",
    "    assert len(result) > 0, \"Response should not be empty\"\n",
    "    points += 1\n",
    "    feedback.append(f\"Function returns non-empty text ({len(result)} chars)\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking response: {type(e).__name__}\")\n",
    "\n",
    "record_score('Exercise 2', points, 3, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-header",
   "metadata": {},
   "source": [
    "## Exercise 3 - JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 3 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'parse_json_response' in dir() or 'parse_json_response' in globals(), \"Function 'parse_json_response' not defined\"\n    assert callable(parse_json_response), \"'parse_json_response' should be a function\"\n    points += 1\n    feedback.append(\"Function 'parse_json_response' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"{e}\")\nexcept Exception as e:\n    feedback.append(f\"Error checking function: {type(e).__name__}\")\n\ntry:\n    result = parse_json_response('Return ONLY this JSON: {\"test\": 1}')\n    assert isinstance(result, tuple), f\"Should return tuple, got {type(result).__name__}\"\n    assert len(result) == 2, f\"Tuple should have 2 elements, got {len(result)}\"\n    points += 1\n    feedback.append(\"Function returns a tuple with 2 elements\")\nexcept AssertionError as e:\n    feedback.append(f\"{e}\")\nexcept Exception as e:\n    feedback.append(f\"Error calling function: {type(e).__name__}\")\n\ntry:\n    success, result = parse_json_response('Return ONLY this JSON: {\"test\": 1}')\n    assert isinstance(success, bool), f\"First element should be bool, got {type(success).__name__}\"\n    points += 1\n    feedback.append(\"First tuple element is a boolean\")\nexcept AssertionError as e:\n    feedback.append(f\"{e}\")\nexcept Exception as e:\n    feedback.append(f\"Error checking tuple: {type(e).__name__}\")\n\ntry:\n    # Test with a prompt that should produce valid JSON\n    # Note: LLMs often wrap in markdown, so student must strip it\n    success, result = parse_json_response('Return ONLY this exact JSON, no other text: {\"value\": 42}')\n    if success:\n        assert isinstance(result, dict), \"On success, second element should be dict\"\n        points += 1\n        feedback.append(\"Correctly parses JSON response (handles markdown wrapping)\")\n    else:\n        # Check if they're at least trying - could be model returning extra text\n        points += 0.5\n        feedback.append(f\"JSON parsing failed: {result}\")\nexcept Exception as e:\n    feedback.append(f\"Error in JSON test: {type(e).__name__}\")\n\nrecord_score('Exercise 3', points, 4, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "ex4-header",
   "metadata": {},
   "source": [
    "## Exercise 4 - Temperature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN TEST: Exercise 4 ===\n",
    "points = 0\n",
    "feedback = []\n",
    "\n",
    "try:\n",
    "    assert 'compare_temperatures' in dir() or 'compare_temperatures' in globals(), \"Function 'compare_temperatures' not defined\"\n",
    "    assert callable(compare_temperatures), \"'compare_temperatures' should be a function\"\n",
    "    points += 1\n",
    "    feedback.append(\"Function 'compare_temperatures' defined\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking function: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = compare_temperatures(\"List 3 colors.\")\n",
    "    assert isinstance(result, dict), f\"Should return dict, got {type(result).__name__}\"\n",
    "    points += 1\n",
    "    feedback.append(\"Function returns a dictionary\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error calling function: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = compare_temperatures(\"List 3 colors.\")\n",
    "    assert 'low_temp' in result, \"Dict should have 'low_temp' key\"\n",
    "    assert 'high_temp' in result, \"Dict should have 'high_temp' key\"\n",
    "    assert 'are_identical' in result, \"Dict should have 'are_identical' key\"\n",
    "    points += 1\n",
    "    feedback.append(\"Dictionary has all required keys\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking keys: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = compare_temperatures(\"List 3 colors.\")\n",
    "    assert isinstance(result['are_identical'], bool), \"'are_identical' should be boolean\"\n",
    "    assert isinstance(result['low_temp'], str), \"'low_temp' should be string\"\n",
    "    assert isinstance(result['high_temp'], str), \"'high_temp' should be string\"\n",
    "    points += 1\n",
    "    feedback.append(\"All values have correct types\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking types: {type(e).__name__}\")\n",
    "\n",
    "record_score('Exercise 4', points, 4, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5-header",
   "metadata": {},
   "source": [
    "## Exercise 5 - Structured Prompt Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN TEST: Exercise 5 ===\n",
    "points = 0\n",
    "feedback = []\n",
    "\n",
    "try:\n",
    "    assert 'build_structured_prompt' in dir() or 'build_structured_prompt' in globals(), \"Function 'build_structured_prompt' not defined\"\n",
    "    assert callable(build_structured_prompt), \"'build_structured_prompt' should be a function\"\n",
    "    points += 1\n",
    "    feedback.append(\"Function 'build_structured_prompt' defined\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking function: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Keep it short\"])\n",
    "    assert isinstance(result, str), f\"Should return str, got {type(result).__name__}\"\n",
    "    points += 1\n",
    "    feedback.append(\"Function returns a string\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error calling function: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Keep it short\"])\n",
    "    assert 'SYSTEM:' in result, \"Should contain 'SYSTEM:' label\"\n",
    "    assert 'TASK:' in result, \"Should contain 'TASK:' label\"\n",
    "    assert 'CONSTRAINTS:' in result, \"Should contain 'CONSTRAINTS:' label\"\n",
    "    points += 1\n",
    "    feedback.append(\"Prompt contains all section labels\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking labels: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = build_structured_prompt(\"Be helpful\", \"Explain X\", [\"Short\", \"Clear\"])\n",
    "    assert '- Short' in result or '- short' in result.lower(), \"Constraints should be prefixed with '- '\"\n",
    "    assert '- Clear' in result or '- clear' in result.lower(), \"Each constraint should have '- ' prefix\"\n",
    "    points += 1\n",
    "    feedback.append(\"Constraints are properly formatted with '- ' prefix\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking constraints: {type(e).__name__}\")\n",
    "\n",
    "try:\n",
    "    result = build_structured_prompt(\"Test system\", \"Test task\", [\"C1\", \"C2\"])\n",
    "    assert 'Test system' in result, \"System instruction should be in output\"\n",
    "    assert 'Test task' in result, \"Task should be in output\"\n",
    "    points += 1\n",
    "    feedback.append(\"All inputs are included in output\")\n",
    "except AssertionError as e:\n",
    "    feedback.append(f\"{e}\")\n",
    "except Exception as e:\n",
    "    feedback.append(f\"Error checking content: {type(e).__name__}\")\n",
    "\n",
    "record_score('Exercise 5', points, 5, feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN: WRITE RESULTS ===\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "result = {\n",
    "    'scores': __assessment_scores,\n",
    "    'feedback': __assessment_feedback,\n",
    "    'timestamp': datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('assessment_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(\"Assessment Results:\")\n",
    "total = sum(s[0] for s in __assessment_scores.values())\n",
    "max_total = sum(s[1] for s in __assessment_scores.values())\n",
    "for exercise, (pts, max_pts) in __assessment_scores.items():\n",
    "    print(f\"  {exercise}: {pts}/{max_pts}\")\n",
    "    if exercise in __assessment_feedback:\n",
    "        for fb in __assessment_feedback[exercise]:\n",
    "            print(f\"    {fb}\")\n",
    "print(f\"\\nTotal: {total}/{max_total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}