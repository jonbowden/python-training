{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c63aed8",
   "metadata": {},
   "source": "# Content\n\n## Overview\nThis module explains **how machine learning and deep learning actually work**, and why modern AI systems\n(including LLMs) behave the way they do.\n\nIt is **conceptual first**, with **light, real code** to ground ideas.\n\nThis module builds on:\n- **Module 1:** Python fundamentals\n- **Module 2:** Data & Pandas\n- **Module 3:** LLM Fundamentals (inference, hallucinations, constraints)\n\nBy the end of this module, the behaviour of LLMs should feel *inevitable*, not mysterious."
  },
  {
   "cell_type": "markdown",
   "id": "1803a50e",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this module, you will be able to:\n",
    "1. Explain AI vs ML vs DL vs Generative AI (GenAI)\n",
    "2. Explain neural network anatomy (neurons, weights, biases, activations, layers)\n",
    "3. Describe how models learn (loss, gradient descent, backpropagation, learning rate, epochs)\n",
    "4. Explain overfitting vs underfitting and why data quality matters\n",
    "5. Understand the “Big Two” DL frameworks: **PyTorch** and **TensorFlow/Keras**\n",
    "6. Connect ML/DL concepts directly to **LLM behaviour**\n",
    "7. Explain why **grounding (RAG)** is required for enterprise reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0bc1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick note on depth\n",
    "You do **not** need to be a mathematician to be an effective AI engineer in a bank. But you **do** need a correct mental model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ef69a",
   "metadata": {},
   "source": [
    "# Group 1 — The Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f856f",
   "metadata": {},
   "source": [
    "## 4.1 AI vs ML vs DL vs GenAI\n",
    "AI is the umbrella. ML learns from data. DL uses neural networks (many layers). GenAI generates new content. LLMs are DL + GenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a9329",
   "metadata": {},
   "source": [
    "## 4.2 Where LLMs Fit\n",
    "LLMs are deep neural networks trained for next-token prediction. They are not databases or truth engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624c892",
   "metadata": {},
   "source": [
    "## 4.3 Supervised vs Unsupervised vs Self-Supervised\n",
    "LLMs are trained mostly via self-supervised learning (labels derived from data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773e62b",
   "metadata": {},
   "source": [
    "## 4.4 Regression vs Classification\n",
    "Regression predicts numbers; classification predicts categories. LLMs internally classify next-token choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7203db79",
   "metadata": {},
   "source": [
    "## 4.5 Training vs Inference\n",
    "Training adjusts weights to reduce loss; inference uses weights to generate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca67073e",
   "metadata": {},
   "source": [
    "# Group 2 — Anatomy of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41680a6",
   "metadata": {},
   "source": [
    "## 4.6 What Is a Neuron?\n",
    "Inputs × weights + bias → activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c343027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def neuron(inputs, weights, bias):\n",
    "    z = sum(x*w for x,w in zip(inputs, weights)) + bias\n",
    "    return 1/(1+math.exp(-z))  # sigmoid\n",
    "print(neuron([1.0,0.5,-1.2],[0.8,-0.3,0.1],0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bb1ab",
   "metadata": {},
   "source": [
    "## 4.7 Weights and Biases\n",
    "Weights are learned importance; bias shifts baseline. Training finds good weights/biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2d600",
   "metadata": {},
   "source": [
    "## 4.8 Activation Functions\n",
    "Activations add non-linearity (ReLU, sigmoid). Without them, networks are linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc683b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def relu(z): return max(0.0, z)\n",
    "def sigmoid(z): return 1/(1+math.exp(-z))\n",
    "for z in [-3,-1,0,1,3]:\n",
    "    print(z, relu(z), round(sigmoid(z),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f950a1a",
   "metadata": {},
   "source": [
    "## 4.9 Layers\n",
    "Input/hidden/output layers transform representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca7d80",
   "metadata": {},
   "source": [
    "## 4.10 What 'Deep' Means\n",
    "Deep means many layers, enabling hierarchical feature learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cfe18",
   "metadata": {},
   "source": [
    "# Group 3 — How Models Learn (Training Mechanics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6049dc5",
   "metadata": {},
   "source": [
    "## 4.11 Loss Functions\n",
    "Loss measures error; training minimises loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f119349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return sum((a-b)**2 for a,b in zip(y_true,y_pred))/len(y_true)\n",
    "print(mse([10,12,13],[9,11,15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8f60a",
   "metadata": {},
   "source": [
    "## 4.12 Gradient Descent (Foggy Mountain)\n",
    "Optimisation is like walking downhill in fog guided by slope (gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b01d30",
   "metadata": {},
   "source": [
    "## 4.13 Backpropagation (High Level)\n",
    "Error flows backward through layers to update weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f37ec",
   "metadata": {},
   "source": [
    "## 4.14 Learning Rate\n",
    "Step size: too small slow; too large unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e508d",
   "metadata": {},
   "source": [
    "## 4.15 Epochs and Convergence\n",
    "Epoch = one full pass through data. Too many → overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207447a",
   "metadata": {},
   "source": [
    "# Group 4 — Reality, Frameworks, and Why This Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6a597",
   "metadata": {},
   "source": [
    "## 4.16 Overfitting vs Underfitting\n",
    "Overfitting memorises; underfitting too simple. Use validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa75c9",
   "metadata": {},
   "source": [
    "## 4.17 Data Quality and Bias\n",
    "Models reflect training data. Poor quality and bias drive poor outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cda2f",
   "metadata": {},
   "source": [
    "## 4.18 PyTorch — Mental Model\n",
    "Imperative, Pythonic, easy to debug (research-favoured)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    x=torch.tensor([1.0,2.0,3.0])\n",
    "    w=torch.tensor([0.1,0.2,0.3])\n",
    "    print((x*w).sum().item())\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not available (ok):\", type(e).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64550e",
   "metadata": {},
   "source": [
    "## 4.19 TensorFlow/Keras — Mental Model\n",
    "Declarative, high-level API; fast to build models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(4, activation=\"relu\", input_shape=(3,)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.summary()\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow not available (ok):\", type(e).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a7fa8",
   "metadata": {},
   "source": [
    "## 4.20 Why This Matters for LLMs (Bridge to RAG)\n",
    "LLMs generalise patterns not truth; hallucination is expected. Grounding/RAG supplies evidence + memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cce208",
   "metadata": {},
   "source": [
    "## Practice Exercises (Ungraded)\n",
    "1) Regression vs classification examples in banking\n",
    "2) Explain learning rate instability\n",
    "3) Explain overfitting in fraud models\n",
    "4) Explain why grounding reduces hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb423d",
   "metadata": {},
   "source": [
    "## Module Summary\n",
    "ML/DL fundamentals explain LLM behaviour; grounding (RAG) is required for enterprise reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}