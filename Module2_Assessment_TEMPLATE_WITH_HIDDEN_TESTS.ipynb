{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcfef08",
   "metadata": {},
   "source": "# Module 2 – Python for Data Work Assessment (Instructor Version)\n\n**Instructor / Grading Template**\n\nThis notebook contains **hidden assessment logic** and must NOT be shared with students.\n\nPurpose:\n- Inject student code programmatically\n- Run automated tests\n- Produce authoritative scores\n\n**Assessment Data Sources:**\nStudents must download real data from:\n1. DJIA from WSJ or Yahoo Finance -> `djia_data.csv`\n2. USD/GBP from FRED (DEXUSUK) -> `fx_usd_gbp.csv`  \n3. Federal Funds Rate from FRED (FEDFUNDS) -> `fed_funds_rate.csv`\n\n**Total Points:** 100 (20 points per task)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN: SCORING SETUP ===\n",
    "__assessment_scores = {}\n",
    "\n",
    "def record_score(exercise, points, max_points):\n",
    "    __assessment_scores[exercise] = (points, max_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN: IMPORTS FOR TESTING ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 10)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1-header",
   "metadata": {},
   "source": [
    "## Task 1 — Load & Inspect DJIA (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Task 1 ===\n# Tests loading REAL DJIA data from djia_data.csv\npoints = 0\n\n# Helper to find column by normalized name\ndef find_col(df, name):\n    for c in df.columns:\n        if c.strip().lower() == name.lower():\n            return c\n    return None\n\ntry:\n    # Check djia_df exists\n    assert 'djia_df' in globals() or 'djia_df' in dir(), \"djia_df not defined\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check required columns (flexible: strip whitespace, case-insensitive)\n    # WSJ data may have leading spaces in column names and no Volume column\n    required_cols = {'date', 'open', 'high', 'low', 'close'}  # Volume is optional\n    actual_cols_normalized = {c.strip().lower() for c in djia_df.columns}\n    assert required_cols.issubset(actual_cols_normalized), f\"Missing required columns. Have: {list(djia_df.columns)}\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check data is sorted by date (oldest first) - find Date column flexibly\n    date_col = find_col(djia_df, 'date')\n    dates = pd.to_datetime(djia_df[date_col])\n    assert dates.is_monotonic_increasing, \"Data not sorted by date (oldest first)\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check Date is datetime type and data has reasonable size\n    date_col = find_col(djia_df, 'date')\n    assert pd.api.types.is_datetime64_any_dtype(djia_df[date_col]), \"Date not datetime type\"\n    assert len(djia_df) >= 100, f\"Data seems too small ({len(djia_df)} rows) - did you load the full CSV?\"\n    points += 5\nexcept:\n    pass\n\nrecord_score('Task 1', points, 20)"
  },
  {
   "cell_type": "markdown",
   "id": "task2-header",
   "metadata": {},
   "source": [
    "## Task 2 — Cleaning & Feature Engineering (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Task 2 ===\n# Tests cleaning and feature engineering on real DJIA data\npoints = 0\n\n# Helper to find column by normalized name\ndef find_col(df, name):\n    for c in df.columns:\n        if c.strip().lower() == name.lower():\n            return c\n    return None\n\ntry:\n    # Check Date is datetime (find column flexibly)\n    date_col = find_col(djia_df, 'date')\n    assert pd.api.types.is_datetime64_any_dtype(djia_df[date_col]), \"Date not datetime\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check Daily_Return column exists (exact name or close match)\n    dr_col = find_col(djia_df, 'daily_return')\n    assert dr_col is not None, \"Daily_Return column missing\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check Daily_Return has values (first row will be NaN from pct_change, rest should have data)\n    dr_col = find_col(djia_df, 'daily_return')\n    non_null_count = djia_df[dr_col].notna().sum()\n    assert non_null_count >= len(djia_df) - 5, f\"Daily_Return has too many NaN values ({non_null_count} valid)\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check Daily_Return is reasonable for real stock data (typically -5% to +5% daily)\n    dr_col = find_col(djia_df, 'daily_return')\n    mean_abs_return = djia_df[dr_col].abs().mean()\n    assert 0.01 < mean_abs_return < 5, f\"Daily_Return mean={mean_abs_return} seems unreasonable for real data\"\n    points += 5\nexcept:\n    pass\n\nrecord_score('Task 2', points, 20)"
  },
  {
   "cell_type": "markdown",
   "id": "task3-header",
   "metadata": {},
   "source": [
    "## Task 3 — Visual Analysis (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Task 3 ===\n# Visual tasks are harder to automate - award points for having the data ready\npoints = 0\n\n# Helper to find column by normalized name\ndef find_col(df, name):\n    for c in df.columns:\n        if c.strip().lower() == name.lower():\n            return c\n    return None\n\ntry:\n    # Check djia_df exists with Close column for plotting\n    assert 'djia_df' in globals()\n    close_col = find_col(djia_df, 'close')\n    assert close_col is not None, \"Close column not found\"\n    assert len(djia_df) > 0\n    points += 10\nexcept:\n    pass\n\ntry:\n    # Check Daily_Return exists for histogram\n    dr_col = find_col(djia_df, 'daily_return')\n    assert dr_col is not None, \"Daily_Return column not found\"\n    assert djia_df[dr_col].notna().sum() > 0\n    points += 10\nexcept:\n    pass\n\nrecord_score('Task 3', points, 20)"
  },
  {
   "cell_type": "markdown",
   "id": "task4-header",
   "metadata": {},
   "source": [
    "## Task 4 — Multi-Dataset Analysis (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Task 4 ===\n# Tests loading REAL FX data from FRED (fx_usd_gbp.csv)\npoints = 0\n\n# Helper to find column by normalized name\ndef find_col(df, name):\n    for c in df.columns:\n        if c.strip().lower() == name.lower():\n            return c\n    return None\n\ntry:\n    # Check fx_df exists\n    assert 'fx_df' in globals(), \"fx_df not defined\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check required columns (flexible matching)\n    date_col = find_col(fx_df, 'date')\n    usd_col = find_col(fx_df, 'usd_gbp')\n    fx_ret_col = find_col(fx_df, 'fx_return')\n    assert date_col is not None, \"Date column missing\"\n    assert usd_col is not None, \"USD_GBP column missing\"\n    assert fx_ret_col is not None, \"FX_Return column missing\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check USD_GBP values are in reasonable range for real FX data (USD/GBP typically 1.1-1.5)\n    usd_col = find_col(fx_df, 'usd_gbp')\n    mean_rate = fx_df[usd_col].mean()\n    assert 1.0 < mean_rate < 1.6, f\"USD_GBP mean={mean_rate} seems unreasonable for real FX data\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check Date is datetime and data has reasonable size\n    date_col = find_col(fx_df, 'date')\n    assert pd.api.types.is_datetime64_any_dtype(fx_df[date_col]), \"Date not datetime\"\n    assert len(fx_df) >= 50, f\"Data seems too small ({len(fx_df)} rows) - did you load the full CSV?\"\n    points += 5\nexcept:\n    pass\n\nrecord_score('Task 4', points, 20)"
  },
  {
   "cell_type": "markdown",
   "id": "task5-header",
   "metadata": {},
   "source": [
    "## Task 5 — Macro Insight (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5-test",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Task 5 ===\n# Tests loading REAL Fed Funds Rate data from FRED (fed_funds_rate.csv)\npoints = 0\n\n# Helper to find column by normalized name\ndef find_col(df, name):\n    for c in df.columns:\n        if c.strip().lower() == name.lower():\n            return c\n    return None\n\ntry:\n    # Check rates_df exists\n    assert 'rates_df' in globals(), \"rates_df not defined\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check FEDFUNDS column exists (flexible matching)\n    ff_col = find_col(rates_df, 'fedfunds')\n    assert ff_col is not None, \"FEDFUNDS column missing\"\n    # Check values are in reasonable range (0-10% historically)\n    max_rate = rates_df[ff_col].max()\n    assert 0 < max_rate < 10, f\"FEDFUNDS max={max_rate} seems unreasonable\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check Date is datetime (flexible matching)\n    date_col = find_col(rates_df, 'date')\n    assert pd.api.types.is_datetime64_any_dtype(rates_df[date_col]), \"Date not datetime\"\n    # Check we have multi-year data (should have 12+ months)\n    assert len(rates_df) >= 12, f\"Data seems too small ({len(rates_df)} rows) - need at least 1 year\"\n    points += 5\nexcept:\n    pass\n\ntry:\n    # Check analysis_text has substantive content\n    assert 'analysis_text' in globals(), \"analysis_text not defined\"\n    # Must have real analysis (more than placeholder text)\n    assert len(analysis_text.strip()) > 200, \"Analysis too short - need 5-8 meaningful sentences\"\n    # Check it doesn't still contain placeholder text\n    assert \"YOUR ANALYSIS HERE\" not in analysis_text, \"Replace placeholder text with your analysis\"\n    points += 5\nexcept:\n    pass\n\nrecord_score('Task 5', points, 20)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIDDEN: WRITE RESULTS ===\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "result = {\n",
    "    'scores': __assessment_scores,\n",
    "    'timestamp': datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('assessment_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(\"Assessment Results:\")\n",
    "total = sum(s[0] for s in __assessment_scores.values())\n",
    "max_total = sum(s[1] for s in __assessment_scores.values())\n",
    "for task, (pts, max_pts) in __assessment_scores.items():\n",
    "    print(f\"  {task}: {pts}/{max_pts}\")\n",
    "print(f\"\\nTotal: {total}/{max_total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}