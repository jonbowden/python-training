{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Module 2: Python for Data Work\n",
    "\n",
    "**CodeVision Python Training**\n",
    "\n",
    "### Contents\n",
    "\n",
    "* **Part 1: Pandas Fundamentals** (Sections 1–4)\n",
    "* **Part 2: Data Manipulation** (Sections 5–8)\n",
    "* **Part 3: Numerical & Statistical Computing** (Sections 9–10)\n",
    "* **Part 4: Data Visualisation** (Sections 11–13)\n",
    "* **Part 5: Advanced Data Sources** (Sections 14–15)\n",
    "\n",
    "---\n",
    "\n",
    "### Welcome to Module 2\n",
    "\n",
    "In Module 1, you learned how Python works. In Module 2, Python starts to feel **useful**.\n",
    "\n",
    "Real data is rarely neat. It arrives incomplete, inconsistent, and spread across files, spreadsheets, databases, and APIs. This module teaches you how to handle that reality calmly and systematically.\n",
    "\n",
    "By the end of this module, you will be confident loading, cleaning, transforming, analysing, and visualising data using Python's most powerful libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Pandas Fundamentals\n",
    "\n",
    "### 1. What Is Pandas and Why We Use It\n",
    "\n",
    "**Pandas** is the most widely used Python library for data analysis. It provides powerful, high-level abstractions for working with tabular data (rows and columns).\n",
    "\n",
    "Think of Pandas as Excel on steroids—but programmable, reproducible, and capable of handling millions of rows.\n",
    "\n",
    "**Key benefits:**\n",
    "* Load data from CSV, Excel, JSON, SQL, and more\n",
    "* Clean and transform messy data\n",
    "* Filter, sort, and aggregate with simple commands\n",
    "* Integrates seamlessly with visualisation libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas with the standard alias 'pd'\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"Pandas is ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### 2. Loading Data from Files\n",
    "\n",
    "Most data analysis starts with loading data from a file. Pandas makes this trivial with functions like `read_csv()` and `read_excel()`.\n",
    "\n",
    "**Common file types:**\n",
    "* `.csv` — Comma-separated values (most common)\n",
    "* `.xlsx` — Excel spreadsheets\n",
    "* `.json` — JavaScript Object Notation\n",
    "* `.parquet` — Efficient columnar storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration\n",
    "sample_data = \"\"\"date,product,region,revenue,units\n",
    "2024-01-01,Widget A,North,1500,30\n",
    "2024-01-01,Widget B,South,2200,44\n",
    "2024-01-02,Widget A,North,1800,36\n",
    "2024-01-02,Widget B,South,1900,38\n",
    "2024-01-03,Widget A,East,2100,42\n",
    "2024-01-03,Widget B,West,,35\n",
    "2024-01-04,Widget A,North,1600,32\n",
    "2024-01-04,Widget B,East,2400,48\"\"\"\n",
    "\n",
    "# Save to a CSV file\n",
    "with open(\"sales.csv\", \"w\") as f:\n",
    "    f.write(sample_data)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"sales.csv\")\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### 3. First Look at a Dataset\n",
    "\n",
    "Before analysing any dataset, you must **inspect it**. This helps you understand:\n",
    "* What columns exist\n",
    "* What data types are present\n",
    "* Whether there are missing values\n",
    "* Basic statistics about the data\n",
    "\n",
    "**Essential inspection methods:**\n",
    "* `df.head()` — First 5 rows\n",
    "* `df.info()` — Column types and missing values\n",
    "* `df.describe()` — Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first few rows\n",
    "print(\"=== First 5 Rows ===\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"=== DataFrame Info ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(\"=== Summary Statistics ===\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 4. DataFrames and Series\n",
    "\n",
    "Pandas has two core data structures:\n",
    "\n",
    "**DataFrame** — A 2D table with rows and columns (like a spreadsheet)\n",
    "\n",
    "**Series** — A single column of data (1D)\n",
    "\n",
    "When you select a single column from a DataFrame, you get a Series. When you select multiple columns, you get a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a single column returns a Series\n",
    "revenue_series = df[\"revenue\"]\n",
    "print(f\"Type: {type(revenue_series)}\")\n",
    "print(revenue_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting multiple columns returns a DataFrame\n",
    "subset_df = df[[\"product\", \"revenue\"]]\n",
    "print(f\"Type: {type(subset_df)}\")\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Manipulation\n",
    "\n",
    "### 5. Filtering Data\n",
    "\n",
    "Filtering allows you to select rows that meet certain conditions. This is one of the most common operations in data analysis.\n",
    "\n",
    "**Syntax:** `df[condition]`\n",
    "\n",
    "The condition is a boolean expression that evaluates to `True` or `False` for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where revenue is greater than 2000\n",
    "high_revenue = df[df[\"revenue\"] > 2000]\n",
    "print(\"=== High Revenue Sales (> 2000) ===\")\n",
    "high_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions with & (and) or | (or)\n",
    "north_high = df[(df[\"region\"] == \"North\") & (df[\"revenue\"] > 1500)]\n",
    "print(\"=== North Region with Revenue > 1500 ===\")\n",
    "north_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 6. Handling Missing Data\n",
    "\n",
    "Real-world data often has missing values. Pandas represents these as `NaN` (Not a Number).\n",
    "\n",
    "**Common strategies:**\n",
    "* `df.isna()` — Detect missing values\n",
    "* `df.fillna(value)` — Replace missing values\n",
    "* `df.dropna()` — Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== Missing Values per Column ===\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing revenue with the column mean\n",
    "df_filled = df.copy()\n",
    "df_filled[\"revenue\"] = df_filled[\"revenue\"].fillna(df_filled[\"revenue\"].mean())\n",
    "print(\"=== After Filling Missing Values ===\")\n",
    "print(df_filled.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### 7. Transforming Data\n",
    "\n",
    "Data transformation means creating new columns or modifying existing ones. This is essential for **feature engineering** — creating useful variables for analysis or machine learning.\n",
    "\n",
    "**Common transformations:**\n",
    "* Calculating derived values (profit = revenue - cost)\n",
    "* Converting data types\n",
    "* Applying functions to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column: revenue per unit\n",
    "df_filled[\"revenue_per_unit\"] = df_filled[\"revenue\"] / df_filled[\"units\"]\n",
    "\n",
    "# Convert date string to datetime\n",
    "df_filled[\"date\"] = pd.to_datetime(df_filled[\"date\"])\n",
    "\n",
    "# Extract day of week\n",
    "df_filled[\"day_of_week\"] = df_filled[\"date\"].dt.day_name()\n",
    "\n",
    "print(\"=== Transformed Data ===\")\n",
    "df_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### 8. Grouping and Aggregation\n",
    "\n",
    "Grouping allows you to split data into categories and calculate summary statistics for each group.\n",
    "\n",
    "**Syntax:** `df.groupby(column)[value_column].aggregate_function()`\n",
    "\n",
    "**Common aggregations:** `sum()`, `mean()`, `count()`, `min()`, `max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total revenue by region\n",
    "print(\"=== Total Revenue by Region ===\")\n",
    "revenue_by_region = df_filled.groupby(\"region\")[\"revenue\"].sum()\n",
    "print(revenue_by_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations\n",
    "print(\"\\n=== Summary by Product ===\")\n",
    "product_summary = df_filled.groupby(\"product\").agg({\n",
    "    \"revenue\": [\"sum\", \"mean\"],\n",
    "    \"units\": \"sum\"\n",
    "})\n",
    "print(product_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Numerical & Statistical Computing\n",
    "\n",
    "### 9. NumPy for Numerical Work\n",
    "\n",
    "**NumPy** is the foundation of scientific computing in Python. It provides fast, memory-efficient arrays and mathematical operations.\n",
    "\n",
    "Pandas is actually built on top of NumPy, so understanding NumPy helps you work more effectively with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a NumPy array\n",
    "revenues = np.array([1500, 2200, 1800, 1900, 2100, 1750, 1600, 2400])\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"Mean: {np.mean(revenues):.2f}\")\n",
    "print(f\"Median: {np.median(revenues):.2f}\")\n",
    "print(f\"Std Dev: {np.std(revenues):.2f}\")\n",
    "print(f\"Min: {np.min(revenues)}, Max: {np.max(revenues)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy with Pandas\n",
    "# Get the underlying NumPy array from a Series\n",
    "revenue_array = df_filled[\"revenue\"].values\n",
    "print(f\"Type: {type(revenue_array)}\")\n",
    "print(f\"Sum using NumPy: {np.sum(revenue_array):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### 10. Statistics with SciPy\n",
    "\n",
    "**SciPy** extends NumPy with advanced statistical functions, including hypothesis tests, distributions, and correlation analysis.\n",
    "\n",
    "**Key function:** `stats.pearsonr(x, y)` — Calculates the Pearson correlation coefficient and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate correlation between revenue and units\n",
    "correlation, p_value = stats.pearsonr(df_filled[\"revenue\"], df_filled[\"units\"])\n",
    "\n",
    "print(f\"Pearson Correlation: {correlation:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if correlation > 0.7:\n",
    "    print(\"Strong positive correlation!\")\n",
    "elif correlation > 0.3:\n",
    "    print(\"Moderate positive correlation.\")\n",
    "else:\n",
    "    print(\"Weak or no correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Data Visualisation\n",
    "\n",
    "### 11. Visualisation with Matplotlib\n",
    "\n",
    "**Matplotlib** is Python's foundational plotting library. It gives you complete control over every aspect of a chart.\n",
    "\n",
    "While it can be verbose, understanding Matplotlib helps you customise any visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Line chart of revenue over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_filled[\"date\"], df_filled[\"revenue\"], marker=\"o\", linewidth=2)\n",
    "plt.title(\"Revenue Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Revenue ($)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of revenue by region\n",
    "plt.figure(figsize=(8, 5))\n",
    "revenue_by_region.plot(kind=\"bar\", color=\"steelblue\", edgecolor=\"black\")\n",
    "plt.title(\"Total Revenue by Region\")\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Total Revenue ($)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### 12. Visualisation with Seaborn\n",
    "\n",
    "**Seaborn** is built on Matplotlib but provides a higher-level interface for statistical visualisations. It produces beautiful charts with less code.\n",
    "\n",
    "Seaborn excels at showing relationships and distributions in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Histogram of revenue distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df_filled[\"revenue\"], bins=10, kde=True, color=\"coral\")\n",
    "plt.title(\"Distribution of Revenue\")\n",
    "plt.xlabel(\"Revenue ($)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot by product\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=df_filled, x=\"product\", y=\"revenue\", palette=\"Set2\")\n",
    "plt.title(\"Revenue Distribution by Product\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with regression line\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.regplot(data=df_filled, x=\"units\", y=\"revenue\", scatter_kws={\"alpha\": 0.7})\n",
    "plt.title(\"Revenue vs Units Sold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### 13. Interactive Charts with Plotly\n",
    "\n",
    "**Plotly** creates interactive visualisations that users can zoom, pan, and hover over to see details. This is especially useful for dashboards and presentations.\n",
    "\n",
    "Plotly Express (`px`) provides a simple interface similar to Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Interactive bar chart\n",
    "fig = px.bar(\n",
    "    df_filled, \n",
    "    x=\"region\", \n",
    "    y=\"revenue\", \n",
    "    color=\"product\",\n",
    "    title=\"Revenue by Region and Product\",\n",
    "    barmode=\"group\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive scatter plot\n",
    "fig = px.scatter(\n",
    "    df_filled,\n",
    "    x=\"units\",\n",
    "    y=\"revenue\",\n",
    "    color=\"region\",\n",
    "    size=\"revenue\",\n",
    "    hover_data=[\"product\", \"date\"],\n",
    "    title=\"Revenue vs Units (Hover for Details)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Advanced Data Sources\n",
    "\n",
    "### 14. Automated Data Profiling\n",
    "\n",
    "For quick exploratory analysis, **ydata-profiling** (formerly pandas-profiling) generates comprehensive HTML reports automatically.\n",
    "\n",
    "This is incredibly useful when you first receive a new dataset and need to understand it quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: ydata-profiling may need to be installed\n",
    "# !pip install ydata-profiling\n",
    "\n",
    "# For large datasets, use minimal mode\n",
    "# from ydata_profiling import ProfileReport\n",
    "# report = ProfileReport(df_filled, title=\"Sales Data Report\", minimal=True)\n",
    "# report.to_notebook_iframe()\n",
    "\n",
    "# For now, let's create a simple profile manually\n",
    "print(\"=== Quick Data Profile ===\")\n",
    "print(f\"Rows: {len(df_filled)}\")\n",
    "print(f\"Columns: {len(df_filled.columns)}\")\n",
    "print(f\"\\nColumn Types:\")\n",
    "print(df_filled.dtypes)\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df_filled.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### 15. Data from APIs and Databases\n",
    "\n",
    "Real-world data often comes from web APIs or databases rather than files.\n",
    "\n",
    "**APIs** — Use the `requests` library to fetch data from web services\n",
    "\n",
    "**Databases** — Use `sqlite3` (built-in) or `sqlalchemy` for database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Example: Fetching data from a public API\n",
    "# Note: This is a demonstration - actual API may vary\n",
    "\n",
    "# Simulated API response\n",
    "api_response = {\n",
    "    \"status\": \"success\",\n",
    "    \"data\": [\n",
    "        {\"symbol\": \"AAPL\", \"price\": 178.50},\n",
    "        {\"symbol\": \"GOOGL\", \"price\": 141.25},\n",
    "        {\"symbol\": \"MSFT\", \"price\": 378.90}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert API response to DataFrame\n",
    "stocks_df = pd.DataFrame(api_response[\"data\"])\n",
    "print(\"=== Stock Data from API ===\")\n",
    "stocks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "# Save our DataFrame to a SQL table\n",
    "df_filled.to_sql(\"sales\", conn, index=False)\n",
    "\n",
    "# Query the database using SQL\n",
    "query = \"\"\"\n",
    "SELECT region, SUM(revenue) as total_revenue\n",
    "FROM sales\n",
    "GROUP BY region\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"=== SQL Query Result ===\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this module, you learned to:\n",
    "\n",
    "1. **Load and inspect data** using Pandas\n",
    "2. **Filter, clean, and transform** datasets\n",
    "3. **Aggregate data** with groupby operations\n",
    "4. **Perform numerical analysis** with NumPy and SciPy\n",
    "5. **Create visualisations** with Matplotlib, Seaborn, and Plotly\n",
    "6. **Work with APIs and databases** as data sources\n",
    "\n",
    "These skills form the foundation for machine learning and AI work in later modules.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Complete the **Module 2 Quiz** to test your understanding\n",
    "2. Work through the **Module 2 Assessment** using real financial data\n",
    "3. Experiment with your own datasets!\n",
    "\n",
    "**Launch Online:** Click the **Rocketship Icon** at the top of this page to launch this notebook in **JupyterLab**, **Jupyter Notebook**, or **Google Colab**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
