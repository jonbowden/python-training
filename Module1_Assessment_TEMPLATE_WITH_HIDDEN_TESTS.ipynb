{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcfef08",
   "metadata": {},
   "source": [
    "# Module 1 – Python Foundations Assessment (Instructor Version)\n",
    "\n",
    "⚠️ **Instructor / Grading Template**\n",
    "\n",
    "This notebook contains **hidden assessment logic** and must NOT be shared with students.\n",
    "\n",
    "Purpose:\n",
    "- Inject student code programmatically\n",
    "- Run automated tests\n",
    "- Produce authoritative scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cd89d",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN: SCORING SETUP ===\n__assessment_scores = {}\n__assessment_feedback = {}\n\ndef record_score(exercise, points, max_points, feedback=None):\n    __assessment_scores[exercise] = (points, max_points)\n    if feedback:\n        __assessment_feedback[exercise] = feedback"
  },
  {
   "cell_type": "markdown",
   "id": "8db9fffc",
   "metadata": {},
   "source": [
    "## Exercise 1 – Variables and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe61aad",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 1 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'name' in dir() or 'name' in globals(), \"Variable 'name' not defined\"\n    assert isinstance(name, str), \"Variable 'name' should be a string\"\n    points += 1\n    feedback.append(\"✓ Variable 'name' is a string\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking 'name': {type(e).__name__}\")\n\ntry:\n    assert 'age' in dir() or 'age' in globals(), \"Variable 'age' not defined\"\n    assert isinstance(age, int), \"Variable 'age' should be an integer\"\n    points += 0.5\n    feedback.append(\"✓ Variable 'age' is an integer\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking 'age': {type(e).__name__}\")\n\ntry:\n    assert 'learning_python' in dir() or 'learning_python' in globals(), \"Variable 'learning_python' not defined\"\n    assert isinstance(learning_python, bool), \"Variable 'learning_python' should be a boolean\"\n    points += 0.5\n    feedback.append(\"✓ Variable 'learning_python' is a boolean\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking 'learning_python': {type(e).__name__}\")\n\nrecord_score('Exercise 1', points, 2, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "a171ea1e",
   "metadata": {},
   "source": [
    "## Exercise 2 – Lists, Loops, and Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2edd678",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 2 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'numbers' in dir() or 'numbers' in globals(), \"Variable 'numbers' not defined\"\n    assert isinstance(numbers, list), \"Variable 'numbers' should be a list\"\n    points += 1\n    feedback.append(\"✓ Variable 'numbers' is a list\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking 'numbers': {type(e).__name__}\")\n\ntry:\n    assert len(numbers) >= 5, f\"List should have at least 5 elements, found {len(numbers)}\"\n    points += 1\n    feedback.append(f\"✓ List has {len(numbers)} elements\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking list length: {type(e).__name__}\")\n\ntry:\n    total = sum(numbers)\n    assert isinstance(total, (int, float)), \"Sum should be a number\"\n    points += 1\n    feedback.append(f\"✓ List contains numbers (sum = {total})\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error summing list: {type(e).__name__}\")\n\ntry:\n    # Check if loop/conditional logic was likely used (evidence of processing)\n    assert any(isinstance(n, (int, float)) for n in numbers), \"List should contain numbers\"\n    points += 1\n    feedback.append(\"✓ List processing verified\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking list contents: {type(e).__name__}\")\n\nrecord_score('Exercise 2', points, 4, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "5e03edd5",
   "metadata": {},
   "source": [
    "## Exercise 3 – Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80f2a5",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 3 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'calculate_average' in dir() or 'calculate_average' in globals(), \"Function 'calculate_average' not defined\"\n    assert callable(calculate_average), \"'calculate_average' should be a function\"\n    points += 1\n    feedback.append(\"✓ Function 'calculate_average' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking function: {type(e).__name__}\")\n\ntry:\n    result = calculate_average([10, 20, 30])\n    assert result == 20, f\"calculate_average([10, 20, 30]) should return 20, got {result}\"\n    points += 1\n    feedback.append(\"✓ calculate_average([10, 20, 30]) = 20\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error testing [10, 20, 30]: {type(e).__name__}\")\n\ntry:\n    result = calculate_average([5])\n    assert result == 5, f\"calculate_average([5]) should return 5, got {result}\"\n    points += 1\n    feedback.append(\"✓ calculate_average([5]) = 5 (single element)\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error testing [5]: {type(e).__name__}\")\n\ntry:\n    result = calculate_average([])\n    assert result is None, f\"calculate_average([]) should return None, got {result}\"\n    points += 1\n    feedback.append(\"✓ calculate_average([]) = None (empty list)\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error testing []: {type(e).__name__}\")\n\nrecord_score('Exercise 3', points, 4, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "515a3d0c",
   "metadata": {},
   "source": [
    "## Exercise 4 – Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5df3b6",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 4 ===\npoints = 0\nfeedback = []\n\ntry:\n    assert 'student_scores' in dir() or 'student_scores' in globals(), \"Variable 'student_scores' not defined\"\n    assert isinstance(student_scores, dict), \"'student_scores' should be a dictionary\"\n    points += 1\n    feedback.append(\"✓ Variable 'student_scores' is a dictionary\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking 'student_scores': {type(e).__name__}\")\n\ntry:\n    assert len(student_scores) >= 3, f\"Dictionary should have at least 3 entries, found {len(student_scores)}\"\n    points += 1\n    feedback.append(f\"✓ Dictionary has {len(student_scores)} entries\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking dictionary size: {type(e).__name__}\")\n\ntry:\n    # Check that keys are strings (names) and values are numbers (scores)\n    assert all(isinstance(k, str) for k in student_scores.keys()), \"Keys should be strings (student names)\"\n    points += 1\n    feedback.append(\"✓ Dictionary keys are strings (names)\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking keys: {type(e).__name__}\")\n\ntry:\n    assert all(isinstance(v, (int, float)) for v in student_scores.values()), \"Values should be numbers (scores)\"\n    avg = sum(student_scores.values()) / len(student_scores)\n    assert avg > 0, \"Average score should be positive\"\n    points += 1\n    feedback.append(f\"✓ Dictionary values are numbers (avg = {avg:.1f})\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking values: {type(e).__name__}\")\n\nrecord_score('Exercise 4', points, 4, feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "23e2624b",
   "metadata": {},
   "source": [
    "## Exercise 5 – File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d2630",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN TEST: Exercise 5 ===\npoints = 0\nfeedback = []\n\n# Create test file\ntest_file = 'server_test_words.txt'\nwith open(test_file, 'w') as f:\n    f.write('apple\\nbanana\\napple\\ncherry\\n')\n\ntry:\n    assert 'analyse_file' in dir() or 'analyse_file' in globals(), \"Function 'analyse_file' not defined\"\n    assert callable(analyse_file), \"'analyse_file' should be a function\"\n    points += 1\n    feedback.append(\"✓ Function 'analyse_file' defined\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking function: {type(e).__name__}\")\n\ntry:\n    result = analyse_file(test_file)\n    assert isinstance(result, tuple), f\"Function should return a tuple, got {type(result).__name__}\"\n    assert len(result) == 3, f\"Tuple should have 3 elements, got {len(result)}\"\n    points += 1\n    feedback.append(\"✓ Function returns a tuple with 3 elements\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking return type: {type(e).__name__}\")\n\ntry:\n    result = analyse_file(test_file)\n    total_words, unique_words, longest_word = result\n    assert total_words == 4, f\"Total words should be 4, got {total_words}\"\n    points += 1\n    feedback.append(f\"✓ Correct total word count: {total_words}\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking total words: {type(e).__name__}\")\n\ntry:\n    result = analyse_file(test_file)\n    total_words, unique_words, longest_word = result\n    assert unique_words == 3, f\"Unique words should be 3, got {unique_words}\"\n    points += 1\n    feedback.append(f\"✓ Correct unique word count: {unique_words}\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking unique words: {type(e).__name__}\")\n\ntry:\n    result = analyse_file(test_file)\n    total_words, unique_words, longest_word = result\n    assert longest_word == 'banana', f\"Longest word should be 'banana', got '{longest_word}'\"\n    points += 2\n    feedback.append(f\"✓ Correct longest word: '{longest_word}'\")\nexcept AssertionError as e:\n    feedback.append(f\"✗ {e}\")\nexcept Exception as e:\n    feedback.append(f\"✗ Error checking longest word: {type(e).__name__}\")\n\nrecord_score('Exercise 5', points, 6, feedback)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277cc15",
   "metadata": {},
   "outputs": [],
   "source": "# === HIDDEN: WRITE RESULTS ===\nimport json\nimport datetime\n\nresult = {\n    'scores': __assessment_scores,\n    'feedback': __assessment_feedback,\n    'timestamp': datetime.datetime.now().isoformat()\n}\n\nwith open('assessment_result.json', 'w') as f:\n    json.dump(result, f, indent=2)\n\nprint(\"Assessment Results:\")\ntotal = sum(s[0] for s in __assessment_scores.values())\nmax_total = sum(s[1] for s in __assessment_scores.values())\nfor exercise, (pts, max_pts) in __assessment_scores.items():\n    print(f\"  {exercise}: {pts}/{max_pts}\")\n    if exercise in __assessment_feedback:\n        for fb in __assessment_feedback[exercise]:\n            print(f\"    {fb}\")\nprint(f\"\\nTotal: {total}/{max_total}\")"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}